{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battle of the Neighborhoods\n",
    "## Model_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Table of Contents__\n",
    "\n",
    "* [Train-test split](#Train-test-split)\n",
    "* [Output labels](#Output-labels)\n",
    "* [Feature-selection](#Feature-selection)\n",
    "* [Logistic regression](#Logistic-regression)\n",
    "* [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://vignette.wikia.nocookie.net/phineasandferb/images/9/9b/Treehouse_Fight.jpg/revision/latest?cb=20090320212850)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For a __Brand New Sports Facility__ in Hyderabad, India\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The story so far...__\n",
    "\n",
    "In the notebook 'intro_data.ipynb', the [motivation](intro_data.ipynb#Introduction) for building a sports facility was outlined. [Data](intro_data.ipynb#Data) was also collected using Google Place API, and stored in JSON files. This data was collected for ten venue categories:\n",
    "[Sports facilities, cinemas, pubs, gyms, banks, hospitals, supermarkets, schools, colleges, and restaurants](intro_data.ipynb#Points-of-interest-in-Hyderabad).\n",
    "\n",
    "In 'preprocessing.ipynb', a large dataset of locations was built. The locations of existing sports facilities was supplemented with locations chosen randomly (uniformly) from the city. For all of these locations, the distances to ten nearest venues for each of the ten venue categories was calculated. This distance data was cast to a pandas DataFrame and subsequently stored to disk to the file 'fullDataset.json'.\n",
    "\n",
    "\n",
    "__In this notebook...__\n",
    "\n",
    "We simple logistic regression. We experiment with feature selection and output labels to find the most appropriate choice. Finally, we interpret the coefficients (for logistic regression at least) to gain insight into the locational dependence of sports venues on other categories.\n",
    "\n",
    "Spoiler alert! Logistic regression, the way I implement it here, feels a bit unsatisfactory. By design, we have a lot more unsuitable locations than suitable locations in the dataset. Labelling them with two classes introduces very significant class imbalance. This makes things a bit messy. Ultimately, while the model does manage to classify locations reasonably well, it leaves a bitter aftertaste. \n",
    "\n",
    "\n",
    "__In the next notebook...__\n",
    "\n",
    "We discard the binary class labels, and keep suitability scores as the target - a real number between 0 and 1. For the continuous target, we first use linear regression and see how it goes. One issue with simple linear regression is that the output may not be neatly bounded by 0 and 1. To fix this, we take the easy route and just throw a feedforward neural network at it with a sigmoid activation at output. We'll try single and multilayer networks. Hopefully, one of these models would do a good job. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time # Will use sleep from time to pause \n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "\n",
    "from warnings import warn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banks', 'cinemas', 'colleges', 'gyms', 'hospitals', 'pubs', 'restaurants', 'schools', 'sports', 'supermarkets']\n",
      "['banks' 'cinemas' 'colleges' 'gyms' 'hospitals' 'pubs' 'restaurants'\n",
      " 'schools' 'sports' 'supermarkets']\n"
     ]
    }
   ],
   "source": [
    "df_full = pd.read_json('fullDataset.json')\n",
    "\n",
    "venueCats = [\"sports\", \"cinemas\", \"pubs\", \"gyms\", \"banks\", \n",
    "             \"hospitals\", \"supermarkets\", \"schools\", \"colleges\", \"restaurants\"]\n",
    "venueCats = sorted(venueCats, key=str.lower)\n",
    "\n",
    "# Because we consider distances to other venues when defining features\n",
    "# and distance to sports facilities when defining output labels and scores\n",
    "venueCats_sans_sports = venueCats.copy()\n",
    "venueCats_sans_sports.remove('sports')\n",
    "\n",
    "# Location of the center of the city: \n",
    "# The location of the \"Buddha Statue\" in a lake called \"Hussain Sagar\" at the center of the city is used\n",
    "latCC = lat_CC = 17.415435\n",
    "lngCC = lng_CC = 78.474296\n",
    "\n",
    "df_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(venueCats)\n",
    "print(df_full.columns[3:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>banks</th>\n",
       "      <th>cinemas</th>\n",
       "      <th>colleges</th>\n",
       "      <th>gyms</th>\n",
       "      <th>hospitals</th>\n",
       "      <th>pubs</th>\n",
       "      <th>restaurants</th>\n",
       "      <th>schools</th>\n",
       "      <th>sports</th>\n",
       "      <th>supermarkets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[17.5333915503, 78.3019480094]</td>\n",
       "      <td>17.533392</td>\n",
       "      <td>78.301948</td>\n",
       "      <td>[0.5944527837, 0.9123336302, 1.1945137594, 1.2...</td>\n",
       "      <td>[2.5226926821999998, 2.7441248317999998, 3.292...</td>\n",
       "      <td>[2.2231445958, 2.4326837456, 2.5586923999, 2.6...</td>\n",
       "      <td>[0.4141267293, 0.6469971331000001, 1.470688927...</td>\n",
       "      <td>[2.4209027735, 2.6356289195000002, 2.651096537...</td>\n",
       "      <td>[0.4326443585, 2.0161228142, 2.0193793055, 2.0...</td>\n",
       "      <td>[2.8365380395, 2.8365380395, 3.6162430067, 4.7...</td>\n",
       "      <td>[0.3709854836, 0.5719475071, 0.5819651468, 0.6...</td>\n",
       "      <td>[0.9887859477000001, 1.5864883539, 1.615919187...</td>\n",
       "      <td>[1.5239640962, 2.0930682862, 2.4115800549, 2.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[17.4942544, 78.1426446]</td>\n",
       "      <td>17.494254</td>\n",
       "      <td>78.142645</td>\n",
       "      <td>[0.6754174845, 0.6909662965000001, 0.746231804...</td>\n",
       "      <td>[4.2429786787, 4.5061204992, 7.7483776384, 8.4...</td>\n",
       "      <td>[0.1316769686, 1.8442535139, 3.9067855433, 3.9...</td>\n",
       "      <td>[0.14670746310000002, 1.0841352373, 1.67283942...</td>\n",
       "      <td>[3.8287129508, 4.5275674007, 4.5310875347, 4.5...</td>\n",
       "      <td>[4.5335069359, 4.7413597513, 8.7463913996, 8.9...</td>\n",
       "      <td>[10.3520028636, 16.9519428463, 16.9519428463, ...</td>\n",
       "      <td>[0.5237399309, 0.5316808758, 0.924419337700000...</td>\n",
       "      <td>[0.0, 0.018022444000000002, 0.0319378584, 0.96...</td>\n",
       "      <td>[13.9095550906, 13.9142012707, 14.3331331326, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[17.2736776385, 78.5670567671]</td>\n",
       "      <td>17.273678</td>\n",
       "      <td>78.567057</td>\n",
       "      <td>[2.2614932277, 2.2624956284, 2.2690632559, 2.2...</td>\n",
       "      <td>[6.4672878001, 6.9237351057000005, 6.956319766...</td>\n",
       "      <td>[1.2465647364, 1.7114100722, 1.9511505964, 2.0...</td>\n",
       "      <td>[1.9511606865000002, 2.1844757094, 2.228745610...</td>\n",
       "      <td>[5.5789316488, 6.0012584187, 6.064016701, 6.44...</td>\n",
       "      <td>[1.2535245812, 3.3873357157, 3.7890114492, 5.2...</td>\n",
       "      <td>[6.7748896167, 7.4194043843, 7.4250739684, 7.4...</td>\n",
       "      <td>[0.9387879407, 1.1159395825, 1.1597221395, 1.2...</td>\n",
       "      <td>[1.6722465827000002, 2.8126944449, 2.976785561...</td>\n",
       "      <td>[2.2234366467, 5.2608094770000005, 5.279426647...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[17.2159971208, 78.5018378083]</td>\n",
       "      <td>17.215997</td>\n",
       "      <td>78.501838</td>\n",
       "      <td>[1.0231317789, 1.0290610733, 1.552344901600000...</td>\n",
       "      <td>[2.8466319478, 7.118468551, 11.137838919, 11.4...</td>\n",
       "      <td>[2.667688001, 2.7771837433, 2.8335145658, 2.84...</td>\n",
       "      <td>[0.8044426591, 0.9004906079, 2.6654642685, 2.7...</td>\n",
       "      <td>[11.6441161148, 11.821180716, 12.5591125675, 1...</td>\n",
       "      <td>[5.8143442376, 7.8437278947, 7.905480393599999...</td>\n",
       "      <td>[3.6821682024999998, 7.8654203973, 7.942906800...</td>\n",
       "      <td>[0.9962343093, 1.0744389745, 1.902801744, 2.60...</td>\n",
       "      <td>[0.6524729677000001, 2.9911387450999998, 4.248...</td>\n",
       "      <td>[4.7284731381, 10.1528607584, 11.1395369905, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[17.5405338, 78.3857611]</td>\n",
       "      <td>17.540534</td>\n",
       "      <td>78.385761</td>\n",
       "      <td>[0.17909229840000002, 0.773952089, 1.214674442...</td>\n",
       "      <td>[4.1980559378, 4.278676515, 4.278676515, 4.480...</td>\n",
       "      <td>[0.17714835180000002, 0.2923846075, 0.76939061...</td>\n",
       "      <td>[0.9023824009, 1.5523972775, 1.5571471251, 2.0...</td>\n",
       "      <td>[2.720175495, 4.302153125, 4.3027458242, 4.588...</td>\n",
       "      <td>[2.2791124241, 3.0928239586, 3.8176102177, 3.8...</td>\n",
       "      <td>[2.2898091591, 3.0622868938, 3.2388035882, 4.5...</td>\n",
       "      <td>[0.0762680701, 0.28190401390000003, 0.36464936...</td>\n",
       "      <td>[0.0, 0.0231518678, 0.0493814418, 0.0514730139...</td>\n",
       "      <td>[2.2811736948, 2.2908107154, 2.2976849177, 2.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Location   Latitude  Longitude  \\\n",
       "0  [17.5333915503, 78.3019480094]  17.533392  78.301948   \n",
       "1        [17.4942544, 78.1426446]  17.494254  78.142645   \n",
       "2  [17.2736776385, 78.5670567671]  17.273678  78.567057   \n",
       "3  [17.2159971208, 78.5018378083]  17.215997  78.501838   \n",
       "4        [17.5405338, 78.3857611]  17.540534  78.385761   \n",
       "\n",
       "                                               banks  \\\n",
       "0  [0.5944527837, 0.9123336302, 1.1945137594, 1.2...   \n",
       "1  [0.6754174845, 0.6909662965000001, 0.746231804...   \n",
       "2  [2.2614932277, 2.2624956284, 2.2690632559, 2.2...   \n",
       "3  [1.0231317789, 1.0290610733, 1.552344901600000...   \n",
       "4  [0.17909229840000002, 0.773952089, 1.214674442...   \n",
       "\n",
       "                                             cinemas  \\\n",
       "0  [2.5226926821999998, 2.7441248317999998, 3.292...   \n",
       "1  [4.2429786787, 4.5061204992, 7.7483776384, 8.4...   \n",
       "2  [6.4672878001, 6.9237351057000005, 6.956319766...   \n",
       "3  [2.8466319478, 7.118468551, 11.137838919, 11.4...   \n",
       "4  [4.1980559378, 4.278676515, 4.278676515, 4.480...   \n",
       "\n",
       "                                            colleges  \\\n",
       "0  [2.2231445958, 2.4326837456, 2.5586923999, 2.6...   \n",
       "1  [0.1316769686, 1.8442535139, 3.9067855433, 3.9...   \n",
       "2  [1.2465647364, 1.7114100722, 1.9511505964, 2.0...   \n",
       "3  [2.667688001, 2.7771837433, 2.8335145658, 2.84...   \n",
       "4  [0.17714835180000002, 0.2923846075, 0.76939061...   \n",
       "\n",
       "                                                gyms  \\\n",
       "0  [0.4141267293, 0.6469971331000001, 1.470688927...   \n",
       "1  [0.14670746310000002, 1.0841352373, 1.67283942...   \n",
       "2  [1.9511606865000002, 2.1844757094, 2.228745610...   \n",
       "3  [0.8044426591, 0.9004906079, 2.6654642685, 2.7...   \n",
       "4  [0.9023824009, 1.5523972775, 1.5571471251, 2.0...   \n",
       "\n",
       "                                           hospitals  \\\n",
       "0  [2.4209027735, 2.6356289195000002, 2.651096537...   \n",
       "1  [3.8287129508, 4.5275674007, 4.5310875347, 4.5...   \n",
       "2  [5.5789316488, 6.0012584187, 6.064016701, 6.44...   \n",
       "3  [11.6441161148, 11.821180716, 12.5591125675, 1...   \n",
       "4  [2.720175495, 4.302153125, 4.3027458242, 4.588...   \n",
       "\n",
       "                                                pubs  \\\n",
       "0  [0.4326443585, 2.0161228142, 2.0193793055, 2.0...   \n",
       "1  [4.5335069359, 4.7413597513, 8.7463913996, 8.9...   \n",
       "2  [1.2535245812, 3.3873357157, 3.7890114492, 5.2...   \n",
       "3  [5.8143442376, 7.8437278947, 7.905480393599999...   \n",
       "4  [2.2791124241, 3.0928239586, 3.8176102177, 3.8...   \n",
       "\n",
       "                                         restaurants  \\\n",
       "0  [2.8365380395, 2.8365380395, 3.6162430067, 4.7...   \n",
       "1  [10.3520028636, 16.9519428463, 16.9519428463, ...   \n",
       "2  [6.7748896167, 7.4194043843, 7.4250739684, 7.4...   \n",
       "3  [3.6821682024999998, 7.8654203973, 7.942906800...   \n",
       "4  [2.2898091591, 3.0622868938, 3.2388035882, 4.5...   \n",
       "\n",
       "                                             schools  \\\n",
       "0  [0.3709854836, 0.5719475071, 0.5819651468, 0.6...   \n",
       "1  [0.5237399309, 0.5316808758, 0.924419337700000...   \n",
       "2  [0.9387879407, 1.1159395825, 1.1597221395, 1.2...   \n",
       "3  [0.9962343093, 1.0744389745, 1.902801744, 2.60...   \n",
       "4  [0.0762680701, 0.28190401390000003, 0.36464936...   \n",
       "\n",
       "                                              sports  \\\n",
       "0  [0.9887859477000001, 1.5864883539, 1.615919187...   \n",
       "1  [0.0, 0.018022444000000002, 0.0319378584, 0.96...   \n",
       "2  [1.6722465827000002, 2.8126944449, 2.976785561...   \n",
       "3  [0.6524729677000001, 2.9911387450999998, 4.248...   \n",
       "4  [0.0, 0.0231518678, 0.0493814418, 0.0514730139...   \n",
       "\n",
       "                                        supermarkets  \n",
       "0  [1.5239640962, 2.0930682862, 2.4115800549, 2.6...  \n",
       "1  [13.9095550906, 13.9142012707, 14.3331331326, ...  \n",
       "2  [2.2234366467, 5.2608094770000005, 5.279426647...  \n",
       "3  [4.7284731381, 10.1528607584, 11.1395369905, 1...  \n",
       "4  [2.2811736948, 2.2908107154, 2.2976849177, 2.3...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using n_nbrs: 10\n"
     ]
    }
   ],
   "source": [
    "n_nbrs = len(df_full.iloc[0,5])  \n",
    "print(\"Using n_nbrs:\",n_nbrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split\n",
    "\n",
    "Let's use 10% for testing. 10% puts the number of testing datapoints at 800, which is a decent number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7227 803\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>banks</th>\n",
       "      <th>cinemas</th>\n",
       "      <th>colleges</th>\n",
       "      <th>gyms</th>\n",
       "      <th>hospitals</th>\n",
       "      <th>pubs</th>\n",
       "      <th>restaurants</th>\n",
       "      <th>schools</th>\n",
       "      <th>sports</th>\n",
       "      <th>supermarkets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[17.5261463694, 78.6276052594]</td>\n",
       "      <td>17.526146</td>\n",
       "      <td>78.627605</td>\n",
       "      <td>[1.1910578882, 2.3531500648, 2.8422613385, 3.7...</td>\n",
       "      <td>[3.8080600188, 3.880950221, 4.6192567462, 4.61...</td>\n",
       "      <td>[0.7143162385, 0.7161157994, 0.720318063800000...</td>\n",
       "      <td>[1.6406338103, 3.8926011936, 4.2390121412, 4.3...</td>\n",
       "      <td>[2.285952825, 6.0126362675, 6.6448280745, 7.92...</td>\n",
       "      <td>[1.3274757278, 4.7781594108, 4.9757089922, 4.9...</td>\n",
       "      <td>[7.0065596931, 8.6121227002, 8.6704252244, 8.7...</td>\n",
       "      <td>[0.8682600604, 1.4718778649, 1.502702441, 1.86...</td>\n",
       "      <td>[0.7575276609, 0.8078827149000001, 0.808729543...</td>\n",
       "      <td>[4.4526784487, 4.4526784487, 4.6963936487, 4.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[17.4896164664, 78.5875253239]</td>\n",
       "      <td>17.489616</td>\n",
       "      <td>78.587525</td>\n",
       "      <td>[0.5046672785, 0.9959750281, 1.098482708, 1.10...</td>\n",
       "      <td>[1.3856085387000001, 1.3856085387000001, 2.075...</td>\n",
       "      <td>[1.1456593896, 1.2360172398, 1.2691097698, 1.3...</td>\n",
       "      <td>[0.5583676247, 1.0898644747, 1.0938081437, 1.2...</td>\n",
       "      <td>[1.1833596464, 2.3888138403, 2.4049978613, 2.4...</td>\n",
       "      <td>[0.7666721712, 1.0510152815, 1.266574856600000...</td>\n",
       "      <td>[1.186694115, 2.7852363335, 3.4320192253, 3.43...</td>\n",
       "      <td>[0.4309211354, 0.5527315308, 0.5540815468, 0.6...</td>\n",
       "      <td>[0.191959, 0.9968841887000001, 1.3476322072, 1...</td>\n",
       "      <td>[0.6596358787000001, 1.1862386071, 1.190026921...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[17.4584430135, 78.3858941153]</td>\n",
       "      <td>17.458443</td>\n",
       "      <td>78.385894</td>\n",
       "      <td>[0.6614147295, 0.9183693181, 0.9243992465, 0.9...</td>\n",
       "      <td>[0.1226588088, 0.6658709516, 0.978313008900000...</td>\n",
       "      <td>[0.2378676964, 0.40345462060000004, 0.76360035...</td>\n",
       "      <td>[0.23238174390000002, 0.6549283723, 0.92749517...</td>\n",
       "      <td>[0.9857354936, 1.4575890019, 1.5033332981, 2.0...</td>\n",
       "      <td>[0.5772582883, 0.8007587541000001, 0.858765659...</td>\n",
       "      <td>[1.0950257596, 1.0974015758, 1.1524645376, 1.3...</td>\n",
       "      <td>[1.1475787796999999, 1.1587734907, 1.188857435...</td>\n",
       "      <td>[0.23238174390000002, 0.2351670223, 0.34204898...</td>\n",
       "      <td>[1.3389279973, 1.3730875276, 1.486202633, 1.61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[17.2098955955, 78.2551144749]</td>\n",
       "      <td>17.209896</td>\n",
       "      <td>78.255114</td>\n",
       "      <td>[4.6863408451, 5.142751971, 5.7310288261, 6.08...</td>\n",
       "      <td>[15.093369544, 15.3850924384, 15.4836151877, 1...</td>\n",
       "      <td>[4.8113402658, 7.3866685385, 7.9082040042, 7.9...</td>\n",
       "      <td>[7.1227889439, 7.4131857985, 7.7316160541, 8.5...</td>\n",
       "      <td>[7.7238078534, 7.7447642134, 7.9383691764, 8.1...</td>\n",
       "      <td>[8.5401378411, 10.7018630694, 14.3107583072, 1...</td>\n",
       "      <td>[14.7280597754, 15.012382467, 15.0239117883, 1...</td>\n",
       "      <td>[1.8165287912, 2.1198085173, 2.6493099229, 2.9...</td>\n",
       "      <td>[7.1532318737, 7.9480794796000005, 9.214021691...</td>\n",
       "      <td>[6.1061942507, 15.2640151772, 15.8625349948, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[17.5933735913, 78.6461721681]</td>\n",
       "      <td>17.593374</td>\n",
       "      <td>78.646172</td>\n",
       "      <td>[3.0000181358, 3.7973513772, 3.9957430046, 3.9...</td>\n",
       "      <td>[5.2367769465, 6.7092104902, 7.9029192477, 7.9...</td>\n",
       "      <td>[0.5844938355, 3.140211194, 3.3731424611, 3.43...</td>\n",
       "      <td>[7.9440239155, 8.013034184, 8.0202853444, 9.00...</td>\n",
       "      <td>[5.4448750133, 10.6512842024, 13.5323509883, 1...</td>\n",
       "      <td>[4.9253467588, 6.4018387935, 8.1228470829, 9.6...</td>\n",
       "      <td>[14.292169597000001, 15.5484860745, 15.6553784...</td>\n",
       "      <td>[3.4885486375, 3.5265950575, 3.9895825396, 4.1...</td>\n",
       "      <td>[3.8898353197, 3.8958615290000003, 8.255870867...</td>\n",
       "      <td>[9.3766983373, 9.3797909069, 9.3935589528, 11....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Location   Latitude  Longitude  \\\n",
       "0  [17.5261463694, 78.6276052594]  17.526146  78.627605   \n",
       "1  [17.4896164664, 78.5875253239]  17.489616  78.587525   \n",
       "2  [17.4584430135, 78.3858941153]  17.458443  78.385894   \n",
       "3  [17.2098955955, 78.2551144749]  17.209896  78.255114   \n",
       "4  [17.5933735913, 78.6461721681]  17.593374  78.646172   \n",
       "\n",
       "                                               banks  \\\n",
       "0  [1.1910578882, 2.3531500648, 2.8422613385, 3.7...   \n",
       "1  [0.5046672785, 0.9959750281, 1.098482708, 1.10...   \n",
       "2  [0.6614147295, 0.9183693181, 0.9243992465, 0.9...   \n",
       "3  [4.6863408451, 5.142751971, 5.7310288261, 6.08...   \n",
       "4  [3.0000181358, 3.7973513772, 3.9957430046, 3.9...   \n",
       "\n",
       "                                             cinemas  \\\n",
       "0  [3.8080600188, 3.880950221, 4.6192567462, 4.61...   \n",
       "1  [1.3856085387000001, 1.3856085387000001, 2.075...   \n",
       "2  [0.1226588088, 0.6658709516, 0.978313008900000...   \n",
       "3  [15.093369544, 15.3850924384, 15.4836151877, 1...   \n",
       "4  [5.2367769465, 6.7092104902, 7.9029192477, 7.9...   \n",
       "\n",
       "                                            colleges  \\\n",
       "0  [0.7143162385, 0.7161157994, 0.720318063800000...   \n",
       "1  [1.1456593896, 1.2360172398, 1.2691097698, 1.3...   \n",
       "2  [0.2378676964, 0.40345462060000004, 0.76360035...   \n",
       "3  [4.8113402658, 7.3866685385, 7.9082040042, 7.9...   \n",
       "4  [0.5844938355, 3.140211194, 3.3731424611, 3.43...   \n",
       "\n",
       "                                                gyms  \\\n",
       "0  [1.6406338103, 3.8926011936, 4.2390121412, 4.3...   \n",
       "1  [0.5583676247, 1.0898644747, 1.0938081437, 1.2...   \n",
       "2  [0.23238174390000002, 0.6549283723, 0.92749517...   \n",
       "3  [7.1227889439, 7.4131857985, 7.7316160541, 8.5...   \n",
       "4  [7.9440239155, 8.013034184, 8.0202853444, 9.00...   \n",
       "\n",
       "                                           hospitals  \\\n",
       "0  [2.285952825, 6.0126362675, 6.6448280745, 7.92...   \n",
       "1  [1.1833596464, 2.3888138403, 2.4049978613, 2.4...   \n",
       "2  [0.9857354936, 1.4575890019, 1.5033332981, 2.0...   \n",
       "3  [7.7238078534, 7.7447642134, 7.9383691764, 8.1...   \n",
       "4  [5.4448750133, 10.6512842024, 13.5323509883, 1...   \n",
       "\n",
       "                                                pubs  \\\n",
       "0  [1.3274757278, 4.7781594108, 4.9757089922, 4.9...   \n",
       "1  [0.7666721712, 1.0510152815, 1.266574856600000...   \n",
       "2  [0.5772582883, 0.8007587541000001, 0.858765659...   \n",
       "3  [8.5401378411, 10.7018630694, 14.3107583072, 1...   \n",
       "4  [4.9253467588, 6.4018387935, 8.1228470829, 9.6...   \n",
       "\n",
       "                                         restaurants  \\\n",
       "0  [7.0065596931, 8.6121227002, 8.6704252244, 8.7...   \n",
       "1  [1.186694115, 2.7852363335, 3.4320192253, 3.43...   \n",
       "2  [1.0950257596, 1.0974015758, 1.1524645376, 1.3...   \n",
       "3  [14.7280597754, 15.012382467, 15.0239117883, 1...   \n",
       "4  [14.292169597000001, 15.5484860745, 15.6553784...   \n",
       "\n",
       "                                             schools  \\\n",
       "0  [0.8682600604, 1.4718778649, 1.502702441, 1.86...   \n",
       "1  [0.4309211354, 0.5527315308, 0.5540815468, 0.6...   \n",
       "2  [1.1475787796999999, 1.1587734907, 1.188857435...   \n",
       "3  [1.8165287912, 2.1198085173, 2.6493099229, 2.9...   \n",
       "4  [3.4885486375, 3.5265950575, 3.9895825396, 4.1...   \n",
       "\n",
       "                                              sports  \\\n",
       "0  [0.7575276609, 0.8078827149000001, 0.808729543...   \n",
       "1  [0.191959, 0.9968841887000001, 1.3476322072, 1...   \n",
       "2  [0.23238174390000002, 0.2351670223, 0.34204898...   \n",
       "3  [7.1532318737, 7.9480794796000005, 9.214021691...   \n",
       "4  [3.8898353197, 3.8958615290000003, 8.255870867...   \n",
       "\n",
       "                                        supermarkets  \n",
       "0  [4.4526784487, 4.4526784487, 4.6963936487, 4.6...  \n",
       "1  [0.6596358787000001, 1.1862386071, 1.190026921...  \n",
       "2  [1.3389279973, 1.3730875276, 1.486202633, 1.61...  \n",
       "3  [6.1061942507, 15.2640151772, 15.8625349948, 1...  \n",
       "4  [9.3766983373, 9.3797909069, 9.3935589528, 11....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_size = 0.1\n",
    "\n",
    "df_train, df_test = train_test_split(df_full, test_size=test_size,random_state=0)\n",
    "\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "print(len(df_train), len(df_test))\n",
    "# Check to make sure that these are pandas dataframes\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, these dataframes just have distances. They do not contain features and output labels. We will experiment with features on labels on df_train (using k-fold cross validation). Features and labels will be defined for df_test only at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sabarish\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>banks</th>\n",
       "      <th>cinemas</th>\n",
       "      <th>colleges</th>\n",
       "      <th>gyms</th>\n",
       "      <th>hospitals</th>\n",
       "      <th>pubs</th>\n",
       "      <th>restaurants</th>\n",
       "      <th>schools</th>\n",
       "      <th>sports</th>\n",
       "      <th>supermarkets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[17.5261463694, 78.6276052594]</td>\n",
       "      <td>17.526146</td>\n",
       "      <td>78.627605</td>\n",
       "      <td>[1.1910578882, 2.3531500648, 2.8422613385, 3.7...</td>\n",
       "      <td>[3.8080600188, 3.880950221, 4.6192567462, 4.61...</td>\n",
       "      <td>[0.7143162385, 0.7161157994, 0.720318063800000...</td>\n",
       "      <td>[1.6406338103, 3.8926011936, 4.2390121412, 4.3...</td>\n",
       "      <td>[2.285952825, 6.0126362675, 6.6448280745, 7.92...</td>\n",
       "      <td>[1.3274757278, 4.7781594108, 4.9757089922, 4.9...</td>\n",
       "      <td>[7.0065596931, 8.6121227002, 8.6704252244, 8.7...</td>\n",
       "      <td>[0.8682600604, 1.4718778649, 1.502702441, 1.86...</td>\n",
       "      <td>[0.7575276609, 0.8078827149000001, 0.808729543...</td>\n",
       "      <td>[4.4526784487, 4.4526784487, 4.6963936487, 4.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[17.4896164664, 78.5875253239]</td>\n",
       "      <td>17.489616</td>\n",
       "      <td>78.587525</td>\n",
       "      <td>[0.5046672785, 0.9959750281, 1.098482708, 1.10...</td>\n",
       "      <td>[1.3856085387000001, 1.3856085387000001, 2.075...</td>\n",
       "      <td>[1.1456593896, 1.2360172398, 1.2691097698, 1.3...</td>\n",
       "      <td>[0.5583676247, 1.0898644747, 1.0938081437, 1.2...</td>\n",
       "      <td>[1.1833596464, 2.3888138403, 2.4049978613, 2.4...</td>\n",
       "      <td>[0.7666721712, 1.0510152815, 1.266574856600000...</td>\n",
       "      <td>[1.186694115, 2.7852363335, 3.4320192253, 3.43...</td>\n",
       "      <td>[0.4309211354, 0.5527315308, 0.5540815468, 0.6...</td>\n",
       "      <td>[0.191959, 0.9968841887000001, 1.3476322072, 1...</td>\n",
       "      <td>[0.6596358787000001, 1.1862386071, 1.190026921...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[17.4584430135, 78.3858941153]</td>\n",
       "      <td>17.458443</td>\n",
       "      <td>78.385894</td>\n",
       "      <td>[0.6614147295, 0.9183693181, 0.9243992465, 0.9...</td>\n",
       "      <td>[0.1226588088, 0.6658709516, 0.978313008900000...</td>\n",
       "      <td>[0.2378676964, 0.40345462060000004, 0.76360035...</td>\n",
       "      <td>[0.23238174390000002, 0.6549283723, 0.92749517...</td>\n",
       "      <td>[0.9857354936, 1.4575890019, 1.5033332981, 2.0...</td>\n",
       "      <td>[0.5772582883, 0.8007587541000001, 0.858765659...</td>\n",
       "      <td>[1.0950257596, 1.0974015758, 1.1524645376, 1.3...</td>\n",
       "      <td>[1.1475787796999999, 1.1587734907, 1.188857435...</td>\n",
       "      <td>[0.23238174390000002, 0.2351670223, 0.34204898...</td>\n",
       "      <td>[1.3389279973, 1.3730875276, 1.486202633, 1.61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[17.2098955955, 78.2551144749]</td>\n",
       "      <td>17.209896</td>\n",
       "      <td>78.255114</td>\n",
       "      <td>[4.6863408451, 5.142751971, 5.7310288261, 6.08...</td>\n",
       "      <td>[15.093369544, 15.3850924384, 15.4836151877, 1...</td>\n",
       "      <td>[4.8113402658, 7.3866685385, 7.9082040042, 7.9...</td>\n",
       "      <td>[7.1227889439, 7.4131857985, 7.7316160541, 8.5...</td>\n",
       "      <td>[7.7238078534, 7.7447642134, 7.9383691764, 8.1...</td>\n",
       "      <td>[8.5401378411, 10.7018630694, 14.3107583072, 1...</td>\n",
       "      <td>[14.7280597754, 15.012382467, 15.0239117883, 1...</td>\n",
       "      <td>[1.8165287912, 2.1198085173, 2.6493099229, 2.9...</td>\n",
       "      <td>[7.1532318737, 7.9480794796000005, 9.214021691...</td>\n",
       "      <td>[6.1061942507, 15.2640151772, 15.8625349948, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[17.5933735913, 78.6461721681]</td>\n",
       "      <td>17.593374</td>\n",
       "      <td>78.646172</td>\n",
       "      <td>[3.0000181358, 3.7973513772, 3.9957430046, 3.9...</td>\n",
       "      <td>[5.2367769465, 6.7092104902, 7.9029192477, 7.9...</td>\n",
       "      <td>[0.5844938355, 3.140211194, 3.3731424611, 3.43...</td>\n",
       "      <td>[7.9440239155, 8.013034184, 8.0202853444, 9.00...</td>\n",
       "      <td>[5.4448750133, 10.6512842024, 13.5323509883, 1...</td>\n",
       "      <td>[4.9253467588, 6.4018387935, 8.1228470829, 9.6...</td>\n",
       "      <td>[14.292169597000001, 15.5484860745, 15.6553784...</td>\n",
       "      <td>[3.4885486375, 3.5265950575, 3.9895825396, 4.1...</td>\n",
       "      <td>[3.8898353197, 3.8958615290000003, 8.255870867...</td>\n",
       "      <td>[9.3766983373, 9.3797909069, 9.3935589528, 11....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Location   Latitude  Longitude  \\\n",
       "0  [17.5261463694, 78.6276052594]  17.526146  78.627605   \n",
       "1  [17.4896164664, 78.5875253239]  17.489616  78.587525   \n",
       "2  [17.4584430135, 78.3858941153]  17.458443  78.385894   \n",
       "3  [17.2098955955, 78.2551144749]  17.209896  78.255114   \n",
       "4  [17.5933735913, 78.6461721681]  17.593374  78.646172   \n",
       "\n",
       "                                               banks  \\\n",
       "0  [1.1910578882, 2.3531500648, 2.8422613385, 3.7...   \n",
       "1  [0.5046672785, 0.9959750281, 1.098482708, 1.10...   \n",
       "2  [0.6614147295, 0.9183693181, 0.9243992465, 0.9...   \n",
       "3  [4.6863408451, 5.142751971, 5.7310288261, 6.08...   \n",
       "4  [3.0000181358, 3.7973513772, 3.9957430046, 3.9...   \n",
       "\n",
       "                                             cinemas  \\\n",
       "0  [3.8080600188, 3.880950221, 4.6192567462, 4.61...   \n",
       "1  [1.3856085387000001, 1.3856085387000001, 2.075...   \n",
       "2  [0.1226588088, 0.6658709516, 0.978313008900000...   \n",
       "3  [15.093369544, 15.3850924384, 15.4836151877, 1...   \n",
       "4  [5.2367769465, 6.7092104902, 7.9029192477, 7.9...   \n",
       "\n",
       "                                            colleges  \\\n",
       "0  [0.7143162385, 0.7161157994, 0.720318063800000...   \n",
       "1  [1.1456593896, 1.2360172398, 1.2691097698, 1.3...   \n",
       "2  [0.2378676964, 0.40345462060000004, 0.76360035...   \n",
       "3  [4.8113402658, 7.3866685385, 7.9082040042, 7.9...   \n",
       "4  [0.5844938355, 3.140211194, 3.3731424611, 3.43...   \n",
       "\n",
       "                                                gyms  \\\n",
       "0  [1.6406338103, 3.8926011936, 4.2390121412, 4.3...   \n",
       "1  [0.5583676247, 1.0898644747, 1.0938081437, 1.2...   \n",
       "2  [0.23238174390000002, 0.6549283723, 0.92749517...   \n",
       "3  [7.1227889439, 7.4131857985, 7.7316160541, 8.5...   \n",
       "4  [7.9440239155, 8.013034184, 8.0202853444, 9.00...   \n",
       "\n",
       "                                           hospitals  \\\n",
       "0  [2.285952825, 6.0126362675, 6.6448280745, 7.92...   \n",
       "1  [1.1833596464, 2.3888138403, 2.4049978613, 2.4...   \n",
       "2  [0.9857354936, 1.4575890019, 1.5033332981, 2.0...   \n",
       "3  [7.7238078534, 7.7447642134, 7.9383691764, 8.1...   \n",
       "4  [5.4448750133, 10.6512842024, 13.5323509883, 1...   \n",
       "\n",
       "                                                pubs  \\\n",
       "0  [1.3274757278, 4.7781594108, 4.9757089922, 4.9...   \n",
       "1  [0.7666721712, 1.0510152815, 1.266574856600000...   \n",
       "2  [0.5772582883, 0.8007587541000001, 0.858765659...   \n",
       "3  [8.5401378411, 10.7018630694, 14.3107583072, 1...   \n",
       "4  [4.9253467588, 6.4018387935, 8.1228470829, 9.6...   \n",
       "\n",
       "                                         restaurants  \\\n",
       "0  [7.0065596931, 8.6121227002, 8.6704252244, 8.7...   \n",
       "1  [1.186694115, 2.7852363335, 3.4320192253, 3.43...   \n",
       "2  [1.0950257596, 1.0974015758, 1.1524645376, 1.3...   \n",
       "3  [14.7280597754, 15.012382467, 15.0239117883, 1...   \n",
       "4  [14.292169597000001, 15.5484860745, 15.6553784...   \n",
       "\n",
       "                                             schools  \\\n",
       "0  [0.8682600604, 1.4718778649, 1.502702441, 1.86...   \n",
       "1  [0.4309211354, 0.5527315308, 0.5540815468, 0.6...   \n",
       "2  [1.1475787796999999, 1.1587734907, 1.188857435...   \n",
       "3  [1.8165287912, 2.1198085173, 2.6493099229, 2.9...   \n",
       "4  [3.4885486375, 3.5265950575, 3.9895825396, 4.1...   \n",
       "\n",
       "                                              sports  \\\n",
       "0  [0.7575276609, 0.8078827149000001, 0.808729543...   \n",
       "1  [0.191959, 0.9968841887000001, 1.3476322072, 1...   \n",
       "2  [0.23238174390000002, 0.2351670223, 0.34204898...   \n",
       "3  [7.1532318737, 7.9480794796000005, 9.214021691...   \n",
       "4  [3.8898353197, 3.8958615290000003, 8.255870867...   \n",
       "\n",
       "                                        supermarkets  \n",
       "0  [4.4526784487, 4.4526784487, 4.6963936487, 4.6...  \n",
       "1  [0.6596358787000001, 1.1862386071, 1.190026921...  \n",
       "2  [1.3389279973, 1.3730875276, 1.486202633, 1.61...  \n",
       "3  [6.1061942507, 15.2640151772, 15.8625349948, 1...  \n",
       "4  [9.3766983373, 9.3797909069, 9.3935589528, 11....  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_train # Make things easier.\n",
    "\n",
    "df.loc[:,venueCats] = df.loc[:,venueCats].applymap(lambda x: np.array(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The premise for the model is that suitability of a location for a sports facility is dependent on its proximity to venues of other categories. This proximity to venues of other categories becomes the input feature vector, $\\boldsymbol{x}$. \n",
    "\n",
    "In a very hand-wavy way, the features will be some function of distances to nearest 'n' venues of each category. Because it makes sense. This function just be a set/vector containing distances to, say, 5 nearest venues of each category for a specified location. Or some weighted sum of distances to 5 nearest venues of each category. Or the distance to the 3rd closest venue of each category. Or something along these lines. The point is, for any location in the city, we must have a method to call its 'n' nearest venues of each category. These nearest venues can then call a second method to produce the features vector. \n",
    "\n",
    "$~$\n",
    "\n",
    "#### Excluding existing sports venues from feature vectors\n",
    "\n",
    "__We will not use distance/proximity to existing sports venues as a feature.__ I admit that it makes a lot of sense to include the distance to existing venues. After all, building a new facility right next to an existing one may not make a lot of business sense. The problem is that this could introduce a lot of unnecessary complexity. Some sports facilities host just one sport, while others are complexes containing a lot of sports halls. Some of these complexes have a single location ID on Google Places, while others have multiple. Some venues are tagged twice or thrice on Google Places. \n",
    "\n",
    "The output label, as we see later, will have something to do with distance to the nearest sports facility. Having a feature and the output label contain the same information introduces some ambiguity. Without being too tedious about this, I will just drop this piece of information from feature vectors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proximity\n",
    "\n",
    "We have data on the 10 closest venues for any specified (latitude,longitude) in Hyderabad. Next, on to defining features.\n",
    "\n",
    "Well, except for one tiny thing. I want to use proximity as the metric instead of distance. How do I define proximity? Easy...\n",
    "$$ \\frac{1}{1+ distance} $$\n",
    "When distance is 0, proximity is 1. For infinite distance, proximity is 0. Simple enough. \n",
    "\n",
    "Why am I doing this? It makes more sense to me. And I don't like seeing lots of negative coefficients in my regression models. \n",
    "\n",
    "Actually, I'll change this just a little bit to\n",
    "$$ \\frac{1}{1 + (distance/0.5)} .$$\n",
    "The factor (1/0.5) for distance is included because 0.5km \"feels\" like a better scaling factor for sizes of neighborhoods than 1km. \n",
    "\n",
    "Later, we use proximity=0.5 as a threshold to decide if a location is suitable or not. Since we use existing sports facilities to set $S=1$, this means that any location within a 0.5km radius of an existing facility is also a suitable location. 0.5km is about a 5 minute walk. This scaling makes sense.\n",
    "\n",
    "__Note: I tried making this definition a bit more flexible by introducing an exponent for (distance/0.5). That made things more complicated without producing a noticeable benefit.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "proximityFun = lambda dist : (1.)/(1.+(dist/0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define four types of features:\n",
    "1. Distance to $n^{th}$ closest venue for each category (excluding sports)\n",
    "2. Distance to first $n$ closest venues for each category (excluding sports)\n",
    "3. Proximity to $n^{th}$ closest venue for each category (excluding sports)\n",
    "4. Proximity to first $n$ closest venues for each category (excluding sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define a function that takes venue category and location as input,\n",
    "#    and returns an associated feature (scalar or vector)\n",
    "# Then, we build a class of such functions\n",
    "\n",
    "def dist_n(dfTmp, venueCat, n=2):\n",
    "    \"\"\" Returns distance to (n+1)^th closest venue of category venueCat for all locations in dfTmp. n Defaults to 2.\n",
    "    n can take values 0 to n_nbrs-1 (inclusive). \"\"\"\n",
    "    return dfTmp[venueCat].apply(lambda x: x[n]).to_numpy().reshape((-1,1))\n",
    "\n",
    "def prox_n(dfTmp, venueCat, n=2):\n",
    "    \"\"\" Returns proximity to (n+1)^th closest venue of category venueCat for all locations in dfTmp. \n",
    "    n defaults to 2. \n",
    "    n can take values 0 to n_nbrs-1 (inclusive) \"\"\"\n",
    "    return dfTmp[venueCat].apply(lambda x: proximityFun(x[n]) ).to_numpy().reshape((-1,1))\n",
    "\n",
    "def dist_0_n(dfTmp, venueCat, n=2):\n",
    "    \"\"\" Returns distance to 1st to (n+1)^th closest venue of category venueCat for all locations in dfTmp. n Defaults to 2.\n",
    "    n can take values 0 to n_nbrs-1 (inclusive). \"\"\"\n",
    "    n = min(n,n_nbrs-1)\n",
    "    return np.array(dfTmp[venueCat].apply(lambda x: x[:n+1] ).to_list()).reshape((-1,n+1))\n",
    "    \n",
    "\n",
    "def prox_0_n(dfTmp, venueCat, n=2):\n",
    "    \"\"\" Returns proximity to 1st to (n+1)^th closest venue of category venueCat for all locations in dfTmp. \n",
    "    n defaults to 2. \n",
    "    n can take values 0 to n_nbrs-1 (inclusive) \"\"\"\n",
    "    n = min(n,n_nbrs-1)\n",
    "    return np.array(dfTmp[venueCat].apply(lambda x: proximityFun(np.array(x[:n+1]) ) ).to_list()).reshape((-1,n+1))\n",
    "\n",
    "\n",
    "def featureVector(dfTmp,featureFun=prox_n, **kwargs):\n",
    "    \"\"\" Returns vector containing features associated to a location \n",
    "    Positional arguments:\n",
    "        lat : Latitude\n",
    "        lng : Longitude\n",
    "    Keyword arguments:\n",
    "        featureFun : Callable that takes arguments (venueCat, **kwargs) to define features\n",
    "                        Defaults to feature_prox_n                    \n",
    "        **kwargs : Passed directly to featureFun\n",
    "    \n",
    "    Returns:\n",
    "        featureVec : pd.Series of shape (m*N,), where m is number of floats returned by featureFun,\n",
    "                        N is len(venueCats)-1\n",
    "    \"\"\"\n",
    "    assert venueCats_sans_sports == sorted(venueCats_sans_sports, key=str.lower)  \n",
    "    # Ensure venueCats_sans_sports properly ordered\n",
    "    \n",
    "       \n",
    "    featureArr0 = featureFun(dfTmp,venueCats_sans_sports[0], **kwargs) # Features for category 0\n",
    "    assert isinstance(featureArr0, np.ndarray) and featureArr0.ndim == 2\n",
    "    m = featureArr0.shape[1]\n",
    "    featureArr = np.zeros( (len(dfTmp), m * (len(venueCats_sans_sports)) ) )  # Initialize array for all cats,except sports\n",
    "    featureArr[:,:m] = featureArr0 # Assign features for cat 0\n",
    "    \n",
    "    # Assign features for other categories\n",
    "    for index in range(1, len(venueCats_sans_sports)):\n",
    "        featureArr[:,index * m : (index+1) * m ] = featureFun(dfTmp,\n",
    "                                venueCats_sans_sports[index], **kwargs)\n",
    "    \n",
    "    return featureArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proximity of 10 closest banks to (17.5261463694,78.6276052594):\n",
      "[[0.2957 0.1752 0.1496 ... 0.1055 0.1049 0.0995]\n",
      " [0.4977 0.3342 0.3128 ... 0.3028 0.2975 0.2955]\n",
      " [0.4305 0.3525 0.351  ... 0.2817 0.2766 0.274 ]\n",
      " ...\n",
      " [0.36   0.1237 0.1086 ... 0.0965 0.0956 0.0916]\n",
      " [0.244  0.2435 0.2359 ... 0.1531 0.1523 0.1516]\n",
      " [0.171  0.1102 0.1089 ... 0.1036 0.1032 0.1031]]\n",
      "Distance of 10 closest banks to (17.5261463694,78.6276052594):\n",
      "[[1.1911 2.3532 2.8423 ... 4.2403 4.2653 4.5253]\n",
      " [0.5047 0.996  1.0985 ... 1.1514 1.1804 1.1918]\n",
      " [0.6614 0.9184 0.9244 ... 1.2748 1.3078 1.325 ]\n",
      " ...\n",
      " [0.8889 3.5411 4.1043 ... 4.6824 4.7276 4.9604]\n",
      " [1.5491 1.5536 1.6192 ... 2.7658 2.7831 2.799 ]\n",
      " [2.4236 4.0371 4.0904 ... 4.3278 4.3461 4.3495]]\n"
     ]
    }
   ],
   "source": [
    "venueCat = \"banks\"\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "print(\"Proximity of {} closest {} to ({},{}):\".format(n_nbrs,venueCat, df.iloc[0,1], df.iloc[0,2]))\n",
    "print( prox_0_n(df,venueCat,n=n_nbrs) )\n",
    "print(\"Distance of {} closest {} to ({},{}):\".format(n_nbrs,venueCat, df.iloc[0,1], df.iloc[0,2]))\n",
    "print( dist_0_n(df,venueCat,n=n_nbrs) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proximity to 3 closest venues...\n",
      "\n",
      "using l=1. in proximity definition\n",
      "[[0.2957 0.1161 0.4118 ... 0.0434 0.2789 0.0754]\n",
      " [0.1752 0.1141 0.4111 ... 0.0427 0.2732 0.0522]\n",
      " [0.1496 0.0977 0.4097 ... 0.0427 0.2394 0.0423]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Proximity to 3 closest venues...\")\n",
    "n=3-1\n",
    "\n",
    "print();print(\"using l=1. in proximity definition\")\n",
    "print(featureVector(df, featureFun=prox_0_n, n=n).reshape(-1,3).T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a simple function to create input feature vectors. The function allows adequate flexibility in terms of the definition of features. Next, the output labels..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output label is the suitability score. We will use the proximity of the closest sports venue as the suitability score; the rationale for this is explained a few cells below. Remember that there is a parameter 'l' in the definition of the proximity. Large values of 'l' produce greater spread in suitability scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3976, 0.7226, 0.6827, ..., 0.1347, 0.2732, 0.3325])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def labelArr(dfTmp):\n",
    "    return prox_n(dfTmp, 'sports',n=0).flatten()\n",
    "\n",
    "labelArr(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output label is the suitability score. I have argued that the fact that existing facilities are located where they are establishes that their locations are suitable. Thus, locations of existing facilities must have a suitability score of 1 (the scale goes from 0 to 1, in line with probability scores of a typical logistic classifier). \n",
    "\n",
    "We must find the functional dependence, given by $f$, \n",
    "$$ f(\\boldsymbol{x}) = S, $$\n",
    "where $\\boldsymbol{x}$ is the input feature vector, and $S$ is the suitability score. \n",
    "\n",
    "This function $f$ is where the machine learning model comes into the picture. Training data is used to create a model that emulates $f$. \n",
    "\n",
    "#### The challenge\n",
    "\n",
    "__The problem is, training data that has locations of only existing sports facilities is inadequate.__ If we train the model using only this data, where all datapoints have an output label of $S=1$, then the most \"accurate\" model would be one which maps every feature vector to $S=1$. Such a model is useless, for obvious reasons. \n",
    "\n",
    "__We need 'negative' training data.__ That is, training data that have suitability scores sufficiently smaller than 1. We assigned $S=1$ using existing facilities. How do we assign $S=0$ to any location and its associated feature vector? If we had a way to do this beforehand, there would be no reason to train a model in the first place. \n",
    "\n",
    "$~$\n",
    "\n",
    "\n",
    "#### In a land far, far away...\n",
    "\n",
    "Let us suppose that, in some hypothetical city, there are a 1000 sports facilities. Let us further suppose that the locations of these facilities are ideally distributed. Well, almost. There is exactly one location in this hypothetical city, call it NewTown, where a sports facility can be profitably operated. \n",
    "\n",
    "Suppose that for any location, existing sports facility or otherwise, we use the proximity to nearest sports facility as the suitability score. This suitability score will be quite appropriate. Because for most locations in the city, it reflects the suitability of such venues. The only locations where the suitability score misrepresents the actual suitability are those around NewTown. NewTown can be a good location to build a new facility, but it ends up being assigned a low suitability score. This can be considered as locations around NewTown being mislabelled. They should have had $S \\approx 1$, but ended up with $S \\approx 0$ because we used proximity to other sports venues.\n",
    "\n",
    "When a model is trained for this city, it has a lot of high quality data, and a tiny set of mislabelled data. This model can be expected to do quite well. The large number of high quality datapoints are bound to eclipse the effects of a few mislabelled datapoints. \n",
    "\n",
    "$~$\n",
    "\n",
    "#### Mislabelling in the training data for Hyderabad\n",
    "\n",
    "In the hypothetical case above, the model is expected to do well when the output label is proximity to the nearest sports facility. Because most locations suitable for such venues were appropriately occupied. This is not the case in Hyderabad. There must be a bunch of locations that are suitable, but are not presently occupied by sports facilities. Let's call such venues HPLs, for High Potential Locations. \n",
    "\n",
    "Ideally, HPLs should be tagged with an output label $S \\approx 1$. The output labels for the training data are going to be labelled with proximity to the nearest existing sports facility as the suitability score $S$. HPLs will have $S$ significantly less than 1, depending on their distance to the nearest facility. Thus, HPLs are bound to be mislabelled in the training data. Locations close to HPLs will also be subject to this mislabelling. \n",
    "\n",
    "The extent of mislabelling in the training data depends on \n",
    "1. The distribution of existing sports facilities - we do not know that they are ideally distributed.\n",
    "2. The number of HPLs in the city. If there are just a handful of these, then the number of mislabelled points would be a lot fewer compared to properly labelled datapoints.\n",
    "\n",
    "In this project, we just make a note of the limitations of the present approach. We shall not investigate the extent of mislabelling or its consequences. We have also not discussed second order effects: if a few more sports facilities are started, how does this affect the profitability of existing facilities? Does the model trained here still hold? These are questions for a different time and place.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and normalisation\n",
    "\n",
    "I have mentioned earlier, quite a few times actually, that __I will not be cleaning the data__. The datasets contain some venues that don't belong in the datasets, and some venues that are missing. There could be some bias in this. I will ignore all of this.\n",
    "\n",
    "Fortunately, __there are no missing values for any of the datapoints__.\n",
    "\n",
    "__There is no need for normalisation__. All of the features belong to the same class - a distance, or a proximity. Some fancy neural networks may require the data to have zero mean, but we are not going to use such models. For the simple models we shall use, the inherent uniformity in the distributions of the features serves the purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colormap: blue for S=0, red for S= 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,<!DOCTYPE html>
<head>    
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <script>L_PREFER_CANVAS=false; L_NO_TOUCH=false; L_DISABLE_3D=false;</script>
    <script src="https://cdn.jsdelivr.net/npm/leaflet@1.4.0/dist/leaflet.js"></script>
    <script src="https://code.jquery.com/jquery-1.12.4.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.4.0/dist/leaflet.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css"/>
    <link rel="stylesheet" href="https://rawcdn.githack.com/python-visualization/folium/master/folium/templates/leaflet.awesome.rotate.css"/>
    <style>html, body {width: 100%;height: 100%;margin: 0;padding: 0;}</style>
    <style>#map {position:absolute;top:0;bottom:0;right:0;left:0;}</style>
    
    <meta name="viewport" content="width=device-width,
        initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <style>#map_9bc2589401034b80af6fe18f4021c15e {
        position: relative;
        width: 100.0%;
        height: 100.0%;
        left: 0.0%;
        top: 0.0%;
        }
    </style>
</head>
<body>    
    
    <div class="folium-map" id="map_9bc2589401034b80af6fe18f4021c15e" ></div>
</body>
<script>    
    
    
        var bounds = null;
    

    var map_9bc2589401034b80af6fe18f4021c15e = L.map(
        'map_9bc2589401034b80af6fe18f4021c15e', {
        center: [17.415435, 78.474296],
        zoom: 11,
        maxBounds: bounds,
        layers: [],
        worldCopyJump: false,
        crs: L.CRS.EPSG3857,
        zoomControl: true,
        });


    
    var tile_layer_37c524c4c526459fa0c379c4079d527c = L.tileLayer(
        'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png',
        {
        "attribution": null,
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_9bc2589401034b80af6fe18f4021c15e);
    
            var circle_marker_34d89925df2e4cb99faac5561e529c70 = L.circleMarker(
                [17.5261463694, 78.6276052594],
                {
  "bubblingMouseEvents": true,
  "color": "#cbcbff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#cbcbff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_7104b22f3c504be38d80068abe042cf0 = L.circleMarker(
                [17.4896164664, 78.5875253239],
                {
  "bubblingMouseEvents": true,
  "color": "#ff8e8e",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff8e8e",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_0340e5265dc94685a7e7d523f2f0f244 = L.circleMarker(
                [17.4584430135, 78.3858941153],
                {
  "bubblingMouseEvents": true,
  "color": "#ffa2a2",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ffa2a2",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_10d39d1c72364751a6a5a7435bf32883 = L.circleMarker(
                [17.2098955955, 78.2551144749],
                {
  "bubblingMouseEvents": true,
  "color": "#2121ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#2121ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_07a65f5f15754207a5a0699680ac34ad = L.circleMarker(
                [17.5933735913, 78.6461721681],
                {
  "bubblingMouseEvents": true,
  "color": "#3a3aff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#3a3aff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_d6263247f36e449a92237e44ee3a014d = L.circleMarker(
                [17.2362535419, 78.3122509949],
                {
  "bubblingMouseEvents": true,
  "color": "#6464ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#6464ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_d5071162427344068cb32d4f0f9ac432 = L.circleMarker(
                [17.4552167233, 78.2542876831],
                {
  "bubblingMouseEvents": true,
  "color": "#3838ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#3838ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_aef06ab176df4d25a033b3eab5d08521 = L.circleMarker(
                [17.4998897903, 78.6111390873],
                {
  "bubblingMouseEvents": true,
  "color": "#8383ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#8383ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_5e098bd9413841e798f1394db7958e5e = L.circleMarker(
                [17.3661597934, 78.2331010226],
                {
  "bubblingMouseEvents": true,
  "color": "#2828ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#2828ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_629420bf986f498cbf899f5822163542 = L.circleMarker(
                [17.4816825895, 78.3920754959],
                {
  "bubblingMouseEvents": true,
  "color": "#ff9e9e",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff9e9e",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_320bff6a3644462fadf0777ba1cdf131 = L.circleMarker(
                [17.5103208964, 78.2277894678],
                {
  "bubblingMouseEvents": true,
  "color": "#dadaff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#dadaff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_93002172e0df4f30a731c352d19cd3b2 = L.circleMarker(
                [17.2870651374, 78.6856002833],
                {
  "bubblingMouseEvents": true,
  "color": "#4a4aff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#4a4aff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_77291f68698a4263bb0c07e53490c151 = L.circleMarker(
                [17.3399853981, 78.6430731933],
                {
  "bubblingMouseEvents": true,
  "color": "#3232ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#3232ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_ea3261888ef742a3a12cd934b2b05861 = L.circleMarker(
                [17.4672872918, 78.3076941757],
                {
  "bubblingMouseEvents": true,
  "color": "#ffd3d3",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ffd3d3",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_0d82db1ff2004f7d982f9247bc0dd146 = L.circleMarker(
                [17.5841730482, 78.3643948338],
                {
  "bubblingMouseEvents": true,
  "color": "#a5a5ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#a5a5ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_dcd8fd31a8974ea384e3e6fd43d61fea = L.circleMarker(
                [17.4135649596, 78.5264554291],
                {
  "bubblingMouseEvents": true,
  "color": "#ffd8d8",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ffd8d8",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_64b3d6168e594cd998f0e2bc36697d04 = L.circleMarker(
                [17.5040637911, 78.2787966951],
                {
  "bubblingMouseEvents": true,
  "color": "#5f5fff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#5f5fff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_9bd6a2adfde544f2bdcebec2a2321921 = L.circleMarker(
                [17.3753124931, 78.2028853899],
                {
  "bubblingMouseEvents": true,
  "color": "#2929ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#2929ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_1b43d3ae1d454db5b709e5bd3eadbe0d = L.circleMarker(
                [17.4003363008, 78.602584292],
                {
  "bubblingMouseEvents": true,
  "color": "#c2c2ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#c2c2ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_beae91a9bda243679fa46fc1d388b7cb = L.circleMarker(
                [17.4800815075, 78.6221626627],
                {
  "bubblingMouseEvents": true,
  "color": "#4d4dff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#4d4dff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_76d2a60fd77e425a9c5ed79cb43b51a6 = L.circleMarker(
                [17.2202967323, 78.690219273],
                {
  "bubblingMouseEvents": true,
  "color": "#1c1cff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#1c1cff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_e0085bb0b3514762809deb62d831b1a6 = L.circleMarker(
                [17.2054621626, 78.4866994802],
                {
  "bubblingMouseEvents": true,
  "color": "#5454ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#5454ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_85039c5ce063498d91c5fe06d950f05d = L.circleMarker(
                [17.4670535206, 78.6012541885],
                {
  "bubblingMouseEvents": true,
  "color": "#dfdfff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#dfdfff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_2493dc3d9a2b491eab6b5a889072de3e = L.circleMarker(
                [17.2277123859, 78.357402785],
                {
  "bubblingMouseEvents": true,
  "color": "#6363ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#6363ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_b1e46bca9b424fbc90aa1c55dc1cd5bb = L.circleMarker(
                [17.5970869084, 78.6229698947],
                {
  "bubblingMouseEvents": true,
  "color": "#6666ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#6666ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_b2c1a4d46f974a10859559cde872a12e = L.circleMarker(
                [17.5499379979, 78.4006412257],
                {
  "bubblingMouseEvents": true,
  "color": "#ff4c4c",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff4c4c",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_4de19e2d5fc84a099434df5632a6f38c = L.circleMarker(
                [17.5528919929, 78.5966137715],
                {
  "bubblingMouseEvents": true,
  "color": "#5555ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#5555ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_512938ef45c84a8aa54850c658126fa1 = L.circleMarker(
                [17.2376124562, 78.3363946882],
                {
  "bubblingMouseEvents": true,
  "color": "#3c3cff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#3c3cff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_07b168f867d24e42a02551a2558a36b4 = L.circleMarker(
                [17.3981029402, 78.1900077589],
                {
  "bubblingMouseEvents": true,
  "color": "#3535ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#3535ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_680faf13cb0f405eb0254be10cc6cadd = L.circleMarker(
                [17.4440752, 78.4878137],
                {
  "bubblingMouseEvents": true,
  "color": "#ff0000",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff0000",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_e8922247e55342cfba27f846b73922da = L.circleMarker(
                [17.2398459721, 78.6699158434],
                {
  "bubblingMouseEvents": true,
  "color": "#2222ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#2222ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_ed23e310bcea415291273a8dba7f1d7f = L.circleMarker(
                [17.5398759506, 78.1681199967],
                {
  "bubblingMouseEvents": true,
  "color": "#5858ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#5858ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_72fd464a8b2a460ba408aabac212b4fc = L.circleMarker(
                [17.2731612855, 78.6714156858],
                {
  "bubblingMouseEvents": true,
  "color": "#2f2fff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#2f2fff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_80f5d8d531da42079410d0fa2f76450c = L.circleMarker(
                [17.4145868, 78.4382677],
                {
  "bubblingMouseEvents": true,
  "color": "#ff0000",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff0000",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_30d016dd4db04609b79b821e7a96a0d4 = L.circleMarker(
                [17.4137350702, 78.3049131763],
                {
  "bubblingMouseEvents": true,
  "color": "#7878ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#7878ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_5455ea785b0446aa873c977dd8ff232c = L.circleMarker(
                [17.4070083795, 78.5142250741],
                {
  "bubblingMouseEvents": true,
  "color": "#f9f9ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#f9f9ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_5889e988528743619b643a5712f51007 = L.circleMarker(
                [17.4155508017, 78.2130234933],
                {
  "bubblingMouseEvents": true,
  "color": "#9b9bff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#9b9bff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_5773d414743c43ab9c8c8750082ae710 = L.circleMarker(
                [17.2511197407, 78.4300679748],
                {
  "bubblingMouseEvents": true,
  "color": "#2222ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#2222ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_39228767ee4940519b85d6587616a075 = L.circleMarker(
                [17.36274505, 78.4070733768],
                {
  "bubblingMouseEvents": true,
  "color": "#cacaff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#cacaff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_f26915a400d349b99ce921101dfa5c62 = L.circleMarker(
                [17.2061625783, 78.3627082848],
                {
  "bubblingMouseEvents": true,
  "color": "#eaeaff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#eaeaff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_4eee4216c1664924b7fd7fc9c3cc6f77 = L.circleMarker(
                [17.4502326, 78.6364892],
                {
  "bubblingMouseEvents": true,
  "color": "#ff0000",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff0000",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_8ab2fa50df824befba46ffd73f080488 = L.circleMarker(
                [17.2800682726, 78.492609696],
                {
  "bubblingMouseEvents": true,
  "color": "#9191ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#9191ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_774786eb9bed4d45820c7c299d33772d = L.circleMarker(
                [17.5337993728, 78.2530946145],
                {
  "bubblingMouseEvents": true,
  "color": "#4747ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#4747ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_1c168d49b1914b03bb9ee5d630b3ac50 = L.circleMarker(
                [17.2580968555, 78.2958188804],
                {
  "bubblingMouseEvents": true,
  "color": "#9e9eff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#9e9eff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_c5aacfa23fd24713bd7f6a30b940ba48 = L.circleMarker(
                [17.3686506, 78.422478],
                {
  "bubblingMouseEvents": true,
  "color": "#ff0000",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff0000",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_646d37b650244b3bb1dc25f15d4a0b98 = L.circleMarker(
                [17.5775458309, 78.6793182056],
                {
  "bubblingMouseEvents": true,
  "color": "#1e1eff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#1e1eff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_ae2ddd0fe61b45e6a10d3e6d1d74ffff = L.circleMarker(
                [17.4104428498, 78.6036739206],
                {
  "bubblingMouseEvents": true,
  "color": "#d5d5ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#d5d5ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_759630bdee514af9a9b4a0d637a8ce65 = L.circleMarker(
                [17.4462147, 78.460599],
                {
  "bubblingMouseEvents": true,
  "color": "#ff0000",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff0000",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_269bc551ea8a4e85ac9c4f7ac1d0dd03 = L.circleMarker(
                [17.5432675661, 78.3281670047],
                {
  "bubblingMouseEvents": true,
  "color": "#4c4cff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#4c4cff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_a6a12277ba874aa99a087c5f868824e6 = L.circleMarker(
                [17.3603650929, 78.1826964115],
                {
  "bubblingMouseEvents": true,
  "color": "#3131ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#3131ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_e49ec48d2893442e835a9f2d823dc58e = L.circleMarker(
                [17.263657484, 78.4213476533],
                {
  "bubblingMouseEvents": true,
  "color": "#2c2cff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#2c2cff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_51f772c3db294dcabd680f2a9c12e330 = L.circleMarker(
                [17.4245926723, 78.4675347708],
                {
  "bubblingMouseEvents": true,
  "color": "#9191ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#9191ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_bd3cb1083cee4769bb09deefc9232bf1 = L.circleMarker(
                [17.5195812656, 78.1928960498],
                {
  "bubblingMouseEvents": true,
  "color": "#3636ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#3636ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_9c6b11d7d615418d9712434f851b21cd = L.circleMarker(
                [17.4261348, 78.453164],
                {
  "bubblingMouseEvents": true,
  "color": "#ff0000",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff0000",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_e4c00270e0234c249f338e0e25db237c = L.circleMarker(
                [17.4548742661, 78.4972860261],
                {
  "bubblingMouseEvents": true,
  "color": "#ffacac",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ffacac",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_0aa3dbe0878140d6b388aee187a1fcdd = L.circleMarker(
                [17.3937674745, 78.5659883644],
                {
  "bubblingMouseEvents": true,
  "color": "#9494ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#9494ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_e7ffa79bd4674f0f89b29a9711dc347f = L.circleMarker(
                [17.313423086, 78.3073811822],
                {
  "bubblingMouseEvents": true,
  "color": "#8a8aff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#8a8aff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_6053a925e22b4b57bfe9250edbbd94f7 = L.circleMarker(
                [17.5993178785, 78.3653222231],
                {
  "bubblingMouseEvents": true,
  "color": "#9696ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#9696ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_4bff87e41eb04ed987518e12c42a5bd2 = L.circleMarker(
                [17.4288457377, 78.3777084616],
                {
  "bubblingMouseEvents": true,
  "color": "#ddddff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ddddff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_3599730bd3c442479a806e0e0a2ff475 = L.circleMarker(
                [17.5824599336, 78.3640804886],
                {
  "bubblingMouseEvents": true,
  "color": "#9898ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#9898ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_d36da2605ac3417381e6e847b15f0eeb = L.circleMarker(
                [17.35802, 78.38421],
                {
  "bubblingMouseEvents": true,
  "color": "#ff0000",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff0000",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_d1c70a9c79d74a31be8e54d3fd13714b = L.circleMarker(
                [17.4854187, 78.4990881],
                {
  "bubblingMouseEvents": true,
  "color": "#ff0000",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff0000",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_397e4e3360444517a2e5c954412e4c28 = L.circleMarker(
                [17.3755463296, 78.2424442552],
                {
  "bubblingMouseEvents": true,
  "color": "#2b2bff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#2b2bff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_5a8f24134c814fbeafa60a40fcd01f4c = L.circleMarker(
                [17.2723280331, 78.399805736],
                {
  "bubblingMouseEvents": true,
  "color": "#3434ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#3434ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_4ff1ea0ad2ba4fc482280068cabfca43 = L.circleMarker(
                [17.4660495435, 78.6560707955],
                {
  "bubblingMouseEvents": true,
  "color": "#7b7bff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#7b7bff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_ce999eb2d1574655aa18fa85e457e57e = L.circleMarker(
                [17.346215783, 78.6644974362],
                {
  "bubblingMouseEvents": true,
  "color": "#3333ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#3333ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_4d9f5927b39d4dda8e06907a36937608 = L.circleMarker(
                [17.5420718859, 78.3379329542],
                {
  "bubblingMouseEvents": true,
  "color": "#6666ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#6666ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_5f54a98097424026ae85db00be627cee = L.circleMarker(
                [17.4436645057, 78.6788663294],
                {
  "bubblingMouseEvents": true,
  "color": "#7373ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#7373ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_11daf943db054b7986f8ece71af8eab6 = L.circleMarker(
                [17.5863270413, 78.6751853148],
                {
  "bubblingMouseEvents": true,
  "color": "#2222ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#2222ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_cf548d79248e48d094d9ff7d9d537a06 = L.circleMarker(
                [17.4580477727, 78.4619269232],
                {
  "bubblingMouseEvents": true,
  "color": "#ff9292",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff9292",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_5ff146a56bd44bab9f6cb22c90070973 = L.circleMarker(
                [17.2467092836, 78.5910615541],
                {
  "bubblingMouseEvents": true,
  "color": "#7070ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#7070ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_900abaea852843d79e12c245d7771850 = L.circleMarker(
                [17.523745988599998, 78.5399695031],
                {
  "bubblingMouseEvents": true,
  "color": "#5252ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#5252ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_61b6342aa2a743ecab7453f450ea521f = L.circleMarker(
                [17.2786329447, 78.3527988439],
                {
  "bubblingMouseEvents": true,
  "color": "#2828ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#2828ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_ef201e0bff2a49a1a5c3eca8667efc41 = L.circleMarker(
                [17.4798337, 78.3181487],
                {
  "bubblingMouseEvents": true,
  "color": "#ff0000",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff0000",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_00dbccfc38c6466c9e2a34f4aea96ef8 = L.circleMarker(
                [17.272879058, 78.2191154449],
                {
  "bubblingMouseEvents": true,
  "color": "#1e1eff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#1e1eff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_be6ec591a7a7428194c1a83a650ef480 = L.circleMarker(
                [17.4835329959, 78.6604851598],
                {
  "bubblingMouseEvents": true,
  "color": "#4949ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#4949ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_6dbbe79974ab49f3b9f71800155baa96 = L.circleMarker(
                [17.4239058402, 78.2978573606],
                {
  "bubblingMouseEvents": true,
  "color": "#6f6fff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#6f6fff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_c2810c30c2c7489393019faca5ee2350 = L.circleMarker(
                [17.3209826392, 78.2803307601],
                {
  "bubblingMouseEvents": true,
  "color": "#eeeeff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#eeeeff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_f426656b855a41bca7d91e076dd1ef05 = L.circleMarker(
                [17.5820186724, 78.688607476],
                {
  "bubblingMouseEvents": true,
  "color": "#1c1cff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#1c1cff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_571531146b7f4579b68d30bef4d84c31 = L.circleMarker(
                [17.2535936035, 78.5964594199],
                {
  "bubblingMouseEvents": true,
  "color": "#4f4fff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#4f4fff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_df38647762e245a68d1c4d1795cffa43 = L.circleMarker(
                [17.5287615634, 78.5352907425],
                {
  "bubblingMouseEvents": true,
  "color": "#4e4eff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#4e4eff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_ff7585e783334141947a6d42e6285f31 = L.circleMarker(
                [17.4998365182, 78.5531506336],
                {
  "bubblingMouseEvents": true,
  "color": "#ff2020",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff2020",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_bfe249fc91494f868a68c3d688e76afe = L.circleMarker(
                [17.2819510772, 78.5955021235],
                {
  "bubblingMouseEvents": true,
  "color": "#4141ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#4141ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_18f6c433949741fda4483f095f236c45 = L.circleMarker(
                [17.2925672596, 78.4124107984],
                {
  "bubblingMouseEvents": true,
  "color": "#6969ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#6969ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_629f24898ae0489ea79a6678f9983a69 = L.circleMarker(
                [17.2862908814, 78.6812497521],
                {
  "bubblingMouseEvents": true,
  "color": "#4545ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#4545ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_5ed39af33ee84e0e8e2c96fe2da173ab = L.circleMarker(
                [17.5070519, 78.4653374],
                {
  "bubblingMouseEvents": true,
  "color": "#ff0000",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff0000",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_2e32b28d9c22459bba8a01fb314f8c1f = L.circleMarker(
                [17.3046586719, 78.446852526],
                {
  "bubblingMouseEvents": true,
  "color": "#5656ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#5656ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_5dd7ca1dbf2b4922b460b96cfdbc0bd1 = L.circleMarker(
                [17.439676, 78.399245],
                {
  "bubblingMouseEvents": true,
  "color": "#ff0000",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff0000",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_1f200b0ef1b349ccb73ad1ebc446365e = L.circleMarker(
                [17.5599144668, 78.1553631073],
                {
  "bubblingMouseEvents": true,
  "color": "#c0c0ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#c0c0ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_f8505bc2b013422fb55318400072533c = L.circleMarker(
                [17.3325486363, 78.2775527239],
                {
  "bubblingMouseEvents": true,
  "color": "#fff3f3",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#fff3f3",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_1db1cacfeb2b4301a0e31feb8a8a2a68 = L.circleMarker(
                [17.5262334334, 78.3421403898],
                {
  "bubblingMouseEvents": true,
  "color": "#bfbfff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#bfbfff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_e599cad3ffa8497280506f2e0a3572b2 = L.circleMarker(
                [17.3673108142, 78.349716817],
                {
  "bubblingMouseEvents": true,
  "color": "#6868ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#6868ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_0fe245eccaa14827becd57b685e64a71 = L.circleMarker(
                [17.4318761239, 78.3010270849],
                {
  "bubblingMouseEvents": true,
  "color": "#b3b3ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#b3b3ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_dd9c8180e23543fab27307cd0dcddd33 = L.circleMarker(
                [17.5856081562, 78.5140718027],
                {
  "bubblingMouseEvents": true,
  "color": "#5858ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#5858ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_3fe4210932db4f0589a7e6e2ba9617bd = L.circleMarker(
                [17.4698715295, 78.3172119052],
                {
  "bubblingMouseEvents": true,
  "color": "#ffeded",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ffeded",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_6627dadc1872488d819771b0eba61085 = L.circleMarker(
                [17.3919944511, 78.3416614468],
                {
  "bubblingMouseEvents": true,
  "color": "#6565ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#6565ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_b17a3ac76b7e48cfa28fc281bdb94bfb = L.circleMarker(
                [17.4240695975, 78.220267599],
                {
  "bubblingMouseEvents": true,
  "color": "#ff3f3f",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ff3f3f",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_29d1a41eb1114508a01ce260e9debdd0 = L.circleMarker(
                [17.591030259, 78.3084507787],
                {
  "bubblingMouseEvents": true,
  "color": "#8f8fff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#8f8fff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_35699615795c40a893b39a4c5f42896e = L.circleMarker(
                [17.3386136703, 78.24946167],
                {
  "bubblingMouseEvents": true,
  "color": "#4e4eff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#4e4eff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
    
            var circle_marker_6141aab33e52498bb3d20a6764458c84 = L.circleMarker(
                [17.5564971708, 78.3853110174],
                {
  "bubblingMouseEvents": true,
  "color": "#e9e9ff",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#e9e9ff",
  "fillOpacity": 0.7,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 3,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_9bc2589401034b80af6fe18f4021c15e);
            
</script>\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x21943f73828>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clamp(x): \n",
    "  return max(0, min(x, 255))\n",
    "\n",
    "def rgb2hex(r,g,b):\n",
    "    return \"#{0:02x}{1:02x}{2:02x}\".format(clamp(r), clamp(g), clamp(b))\n",
    "\n",
    "def blueRed(x):\n",
    "    assert 0<= x <= 1\n",
    "    if x <= 0.5:\n",
    "        r = int(512*x)\n",
    "        b = 256\n",
    "        g = int(512*x)\n",
    "    else :\n",
    "        r = 256\n",
    "        b = int((1.-x)*512)\n",
    "        g = int((1.-x)*512)\n",
    "    return rgb2hex(r,g,b)\n",
    "\n",
    "# Plot first 100 locations in dataset\n",
    "map_tmp = folium.Map(\n",
    "    location=[lat_CC, lng_CC], zoom_start=11)\n",
    "\n",
    "y = labelArr(df)\n",
    "for ind in range(100):\n",
    "    folium.CircleMarker(\n",
    "        [df.iloc[ind,1], df.iloc[ind,2]],\n",
    "        radius=3,\n",
    "        fill=True,\n",
    "        fill_color=blueRed(y[ind]),\n",
    "        color = blueRed(y[ind]),\n",
    "        fill_opacity=0.7).add_to(map_tmp)\n",
    "\n",
    "print(\"Colormap: blue for S=0, red for S= 1\")    \n",
    "map_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we have is labelled such that locations close to city center have large suitability scores (red in the map) and those far away have small suitability scores (blue in the map). There are some datapoints with large suitability scores located away from the city though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined four types of features. Before we run them all, let's do a simple case of logistic regression. For this simple case, we'll use proximity to 3rd nearest venue (n=2). \n",
    "\n",
    "We will use L1 penalty (instead of L2) in the hope that some features drop out, at least when we use lots of features later. L2 is supposedly better behaved, but for the small number of features we use, this may not be a significant issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = featureVector(df, featureFun=prox_n, n=2)\n",
    "y = prox_n(df, 'sports', n=0)\n",
    "yBool = (y>=0.5).astype(int)\n",
    "yBool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive datapoints in training set: 1339\n",
      "Number of negative datapoints in training set: 5888\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of positive datapoints in training set:\", np.sum(yBool))\n",
    "print(\"Number of negative datapoints in training set:\", yBool.size-np.sum(yBool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y is supposed to act as a probability score, and yBool contains actual labels. Note that we used 0.5 as the threshold here. Proximity, the way we defined it, becomes 0.5 when the distance is 0.5km. The threshold of 0.5 essentially means that we are assigning any location within a 0.5km radius of an existing facility as being suitable.\n",
    "\n",
    "The number of negative datapoints is significantly greater than the number of positive datapoints. This is by design. We have a lot of datapoints obtained by random sampling. Most of these should not fall into suitable locations. This class imbalance is something we have to deal with. We'll let scikit-learn impose class weights to do the balancing for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients from logistic regression:\n",
      "banks        :  1.0406\n",
      "cinemas      : -1.8353\n",
      "colleges     :  4.4077\n",
      "gyms         :  2.7181\n",
      "hospitals    : -1.1459\n",
      "pubs         :  0.7910\n",
      "restaurants  :  3.3674\n",
      "schools      :  0.0715\n",
      "supermarkets :  4.2064\n",
      "\n",
      "Number of actual positives in testing set: 163\n",
      "Number of actual negatives in testing set: 640\n",
      "Accuracy of majority classifier:\n",
      "0.797011207970112\n",
      "\n",
      "Jaccard similarity score:\n",
      "0.813200498132005\n",
      "\n",
      "Log loss:\n",
      "0.462723184430015\n",
      "\n",
      "f1 score:\n",
      "0.6445497630331753\n",
      "\n",
      "Confusion matrix:\n",
      "[[517 123]\n",
      " [ 27 136]]\n",
      "\n",
      "Average precision:\n",
      "0.11964071017054116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sabarish\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Consider writing a helper function to calculate p-values\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '2-class Precision-Recall curve: AP=0.12')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, f1_score, log_loss\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, jaccard_similarity_score\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "# Split venues into suitable and unsuitable, right down the middle. Weight classes so they're balanced.\n",
    "logisticModel = LogisticRegression(penalty='l1',solver='liblinear',class_weight='balanced').fit(X, yBool.flatten()) \n",
    "\n",
    "print(\"Coefficients from logistic regression:\")\n",
    "for ind in range(len(venueCats_sans_sports)):\n",
    "    print(\"{:13s}: {:7.4f}\".format(venueCats_sans_sports[ind], logisticModel.coef_[0][ind]))\n",
    "    \n",
    "warn(\"Consider writing a helper function to calculate p-values\")\n",
    "\n",
    "# Testing\n",
    "X_test = featureVector(df_test, featureFun=prox_n, n=2)\n",
    "y_test = prox_n(df_test, 'sports', n=0).flatten()\n",
    "yBool_test = (y_test>=0.5).astype(int).flatten()\n",
    "\n",
    "predictions = logisticModel.predict(X_test)\n",
    "prob_predictions = logisticModel.predict_proba(X_test)\n",
    "\n",
    "nPos = np.sum(yBool_test)\n",
    "nNeg = yBool_test.size - nPos\n",
    "print()\n",
    "print(\"Number of actual positives in testing set:\", nPos)\n",
    "print(\"Number of actual negatives in testing set:\", nNeg)\n",
    "print(\"Accuracy of majority classifier:\")\n",
    "print(max(nPos,nNeg)/yBool_test.size)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Jaccard similarity score:\")\n",
    "print(jaccard_similarity_score(yBool_test, predictions))\n",
    "\n",
    "print()\n",
    "print(\"Log loss:\")\n",
    "print(log_loss(yBool_test, prob_predictions))\n",
    "\n",
    "print()\n",
    "print(\"f1 score:\")\n",
    "print(f1_score(yBool_test, predictions))\n",
    "\n",
    "print()\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(yBool_test, predictions))\n",
    "\n",
    "print()\n",
    "print(\"Average precision:\")\n",
    "average_precision = average_precision_score(yBool_test, prob_predictions[:,0])\n",
    "print(average_precision)\n",
    "\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(yBool_test, prob_predictions[:,0])\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple model seems to do fairly well. \n",
    "\n",
    "81% Jaccard similarity score as opposed to 80% for majority classifier. Well, what do we expect for data that is so imbalanced?\n",
    "\n",
    "The f1 score is 64%. We haven't decided on the relative costs of false negatives and false positives. Either way, 64% for this simple model isn't bad. \n",
    "\n",
    "The confusion matrix looks good as well. For the minority class (suitable locations), most of the identified locations are suitable indeed. \n",
    "\n",
    "The average precision score and precision-recall curve don't look so good. That doesn't necessarily mean the model is useless. These metrics describe the model's behavior with varying threshold for classification. For the typical classifier, these numbers indeed look bad. The present classifier is a bit unusual though - the labels that we have used are a bit dodgy. The labels themselves are dependent on thresholding the proximity to nearest sports facility. It's best not to pay too much attention to these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built the previous model using proximitiy to 3rd closest venues. Why 3rd? Well, let's find out which set of features do the best job. \n",
    "\n",
    "Let us think of 4 different sets of models for simple logistic regression with linear features:\n",
    "1. Using proximity to $n^{th}$ nearest venue\n",
    "2. Using proximity for first to $n^{th}$ nearest venues\n",
    "3. Using distance to $n^{th}$ nearest venue\n",
    "4. Using distance for first to $n^{th}$ nearest venues\n",
    "\n",
    "The function that chooses these features (and the parameters l and n that go into it) is considered a hyperparameter for the logistic regression model.\n",
    "\n",
    "How do we optimize the hyperparameter(s)? Use k-fold cross validation. Let's use k=5, because, reasons. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [08:07<00:00, 80.76s/it] \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "cv = 5 # Number of folds for k-fold cross validation\n",
    "thres = 0.5  # Threshold to use for classification\n",
    "nArr = np.arange(n_nbrs)\n",
    "featFunList = [dist_n, dist_0_n, prox_n, prox_0_n]\n",
    "\n",
    "cvalScoreArr_acc = np.zeros((len(featFunList)+1, nArr.size, cv))\n",
    "cvalScoreArr_f1 = np.zeros((len(featFunList)+1, nArr.size, cv))\n",
    "\n",
    "\n",
    "logisticModel_untrained = LogisticRegression(penalty='l1',solver='liblinear',class_weight='balanced')\n",
    "for i0 in tqdm(range(len(featFunList))):\n",
    "    featFun = featFunList[i0]\n",
    "    for i1 in range(nArr.size):\n",
    "        n = nArr[i1]\n",
    "    \n",
    "        X = featureVector(df_train, featureFun=featFun, n=n)\n",
    "        y = labelArr(df_train)\n",
    "        yBool = (y>=thres).astype(int)\n",
    "        # Label is always for closest sports facility, l could vary\n",
    "        cvalScore_acc = cross_val_score(logisticModel_untrained, X, yBool, cv=cv, scoring='accuracy') # Use accuracy\n",
    "        cvalScoreArr_acc[i0,i1] = cvalScore_acc\n",
    "        cvalScore_f1 = cross_val_score(logisticModel_untrained, X, yBool, cv=cv, scoring='f1') # Use f1_score\n",
    "        cvalScoreArr_f1[i0,i1] = cvalScore_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define majority classifier for comparison\n",
    "majClass = DummyClassifier(strategy='most_frequent')\n",
    "X = featureVector(df_train, featureFun=dist_n, n=n) # Doesn't matter for majority classifier\n",
    "y = labelArr(df_train)\n",
    "yBool = (y>=thres).astype(int)\n",
    "# Label is always for closest sports facility, l could vary\n",
    "cvalScore_majClass = np.mean( cross_val_score(majClass, X, yBool, cv=cv, scoring='accuracy') ) # Use f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy for majority class: 0.815\n",
      "\n",
      "Average cross validation scores (accuracy)...\n",
      "     dist_n   | dist_0_n | prox_n   | prox_0_n \n",
      "              |          |          |          \n",
      "n= 0:  0.760  |  0.760   |  0.817   |  0.817\n",
      "n= 1:  0.754  |  0.772   |  0.818   |  0.826\n",
      "n= 2:  0.743  |  0.776   |  0.818   |  0.834\n",
      "n= 3:  0.757  |  0.780   |  0.819   |  0.835\n",
      "n= 4:  0.755  |  0.783   |  0.817   |  0.835\n",
      "n= 5:  0.755  |  0.790   |  0.816   |  0.835\n",
      "n= 6:  0.756  |  0.794   |  0.816   |  0.833\n",
      "n= 7:  0.756  |  0.794   |  0.813   |  0.832\n",
      "n= 8:  0.765  |  0.793   |  0.813   |  0.832\n",
      "n= 9:  0.764  |  0.795   |  0.811   |  0.832\n",
      "\n",
      "Average cross validation scores (f1)...\n",
      "     dist_n   | dist_0_n | prox_n   | prox_0_n \n",
      "              |          |          |          \n",
      "n= 0:  0.579  |  0.579   |  0.625   |  0.625\n",
      "n= 1:  0.568  |  0.592   |  0.625   |  0.641\n",
      "n= 2:  0.554  |  0.596   |  0.621   |  0.655\n",
      "n= 3:  0.564  |  0.599   |  0.620   |  0.656\n",
      "n= 4:  0.558  |  0.605   |  0.618   |  0.656\n",
      "n= 5:  0.559  |  0.610   |  0.614   |  0.657\n",
      "n= 6:  0.558  |  0.614   |  0.613   |  0.652\n",
      "n= 7:  0.557  |  0.612   |  0.607   |  0.652\n",
      "n= 8:  0.567  |  0.611   |  0.607   |  0.651\n",
      "n= 9:  0.565  |  0.612   |  0.604   |  0.652\n"
     ]
    }
   ],
   "source": [
    "featFunListStr = ['dist_n', 'dist_0_n', 'prox_n', 'prox_0_n']\n",
    "\n",
    "print(\"Average accuracy for majority class: {:4.3f}\".format(cvalScore_majClass))\n",
    "\n",
    "meanScores = np.mean(cvalScoreArr_acc,axis=-1)\n",
    "print()\n",
    "print(\"Average cross validation scores (accuracy)...\")\n",
    "print(\"     {:8s} | {:8s} | {:8s} | {:8s} \".format(*tuple(featFunListStr)) )\n",
    "print(\"              |          |          |          \")\n",
    "for i1 in range(nArr.size):\n",
    "    printStr = \"n={:2d}:\".format(nArr[i1]) + \\\n",
    "            \"  {:4.3f}  |\".format(meanScores[0,i1]) + \\\n",
    "            \"  {:4.3f}   |\".format(meanScores[1,i1]) + \\\n",
    "            \"  {:4.3f}   |\".format(meanScores[2,i1]) + \\\n",
    "            \"  {:4.3f}\".format(meanScores[3,i1])\n",
    "            \n",
    "    print(printStr)\n",
    "\n",
    "meanScores = np.mean(cvalScoreArr_f1,axis=-1)\n",
    "print()\n",
    "print(\"Average cross validation scores (f1)...\")\n",
    "print(\"     {:8s} | {:8s} | {:8s} | {:8s} \".format(*tuple(featFunListStr)) )\n",
    "print(\"              |          |          |          \")\n",
    "for i1 in range(nArr.size):\n",
    "    printStr = \"n={:2d}:\".format(nArr[i1]) + \\\n",
    "            \"  {:4.3f}  |\".format(meanScores[0,i1]) + \\\n",
    "            \"  {:4.3f}   |\".format(meanScores[1,i1]) + \\\n",
    "            \"  {:4.3f}   |\".format(meanScores[2,i1]) + \\\n",
    "            \"  {:4.3f}\".format(meanScores[3,i1])\n",
    "            \n",
    "    print(printStr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things are a bit messy now. We tried to balance the dataset, but now we have accuracies that aren't better than a majority classifier. This isn't unexpected, but makes it hard to interpret results. The f1 scores aren't too bad, but they don't go together with the accuracy. \n",
    "\n",
    "Let's drop datapoints causing the imbalance. We will keep proximity with $l=1$ as the output label, and just drop all of the datapoints in the training set that are neither here nor there - ones with $0.3\\leq S <0.5$. Don't worry about the test set for now, because we're doing cross validation. Reduce the lower bound if that's not enough. It should still leave us with about 3000 data points, which isn't too little. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop entries for 0.100<=y<0.5... Number of entries to drop :4698\n"
     ]
    }
   ],
   "source": [
    "drop_thres = 0.475 # Start here, and drop in steps of 0.025\n",
    "y0 = labelArr(df)\n",
    "nNeg0 = np.sum(y0 <0.5)\n",
    "nPos0 = np.sum(y0>=0.5)\n",
    "\n",
    "# Keep looping as long as nNeg doesn't drop below 1.1*nPos0\n",
    "while True:\n",
    "    dropInds = np.logical_and( (drop_thres <= y0) , (y0 < 0.5) )\n",
    "    nDrop =  np.sum(dropInds)\n",
    "    if (nNeg0-nDrop) <= 1.1*nPos0 :\n",
    "        break\n",
    "    else :\n",
    "        drop_thres -= 0.025\n",
    "\n",
    "dropInds = dropInds.flatten()\n",
    "print(\"Drop entries for {:.3f}<=y<0.5... Number of entries to drop :{}\".format(drop_thres, nDrop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not surprising that we have to drop so many entries. Let's use dropInds later whenever needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:02<00:00,  3.19it/s]\n",
      "100%|| 10/10 [00:32<00:00,  4.46s/it]\n",
      "100%|| 10/10 [00:01<00:00,  6.86it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "cv = 5 # Number of folds for k-fold cross validation\n",
    "thres = 0.5  # Threshold to use for classification\n",
    "nArr = np.arange(n_nbrs)\n",
    "featFunList = [dist_n, dist_0_n, prox_n, prox_0_n]\n",
    "\n",
    "cvalScoreArr_acc = np.zeros((len(featFunList)+1, nArr.size, cv))\n",
    "cvalScoreArr_f1 = np.zeros((len(featFunList)+1, nArr.size, cv))\n",
    "\n",
    "\n",
    "logisticModel_untrained = LogisticRegression(penalty='l1',solver='liblinear',class_weight='balanced')\n",
    "for i0 in range(len(featFunList)):\n",
    "    featFun = featFunList[i0]\n",
    "    for i1 in tqdm(range(nArr.size)):\n",
    "        n = nArr[i1]\n",
    "\n",
    "        X = featureVector(df_train, featureFun=featFun, n=n)[~dropInds,:]\n",
    "        y = y0[~dropInds]\n",
    "        yBool = (y>=thres).astype(int)\n",
    "        # Label is always for closest sports facility, l could vary\n",
    "        cvalScore_acc = cross_val_score(logisticModel_untrained, X, yBool, cv=cv, scoring='accuracy') # Use accuracy\n",
    "        cvalScoreArr_acc[i0,i1] = cvalScore_acc\n",
    "        cvalScore_f1 = cross_val_score(logisticModel_untrained, X, yBool, cv=cv, scoring='f1') # Use f1_score\n",
    "        cvalScoreArr_f1[i0,i1] = cvalScore_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define majority classifier for comparison\n",
    "majClass = DummyClassifier(strategy='most_frequent')\n",
    "X = featureVector(df_train, featureFun=dist_n, n=n)[~dropInds,:] # Doesn't matter for majority classifier\n",
    "y = labelArr(df_train)[~dropInds]\n",
    "yBool = (y>=thres).astype(int)\n",
    "# Label is always for closest sports facility, l could vary\n",
    "cvalScore_majClass = np.mean( cross_val_score(majClass, X, yBool, cv=cv, scoring='accuracy') ) # Use f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy for majority class: 0.529\n",
      "\n",
      "Average cross validation scores (accuracy)...\n",
      "     dist_n   | dist_0_n | prox_n   | prox_0_n \n",
      "              |          |          |          \n",
      "n= 0:  0.942  |  0.942   |  0.957   |  0.957\n",
      "n= 1:  0.951  |  0.975   |  0.960   |  0.966\n",
      "n= 2:  0.959  |  0.980   |  0.959   |  0.970\n",
      "n= 3:  0.951  |  0.981   |  0.948   |  0.970\n",
      "n= 4:  0.947  |  0.985   |  0.943   |  0.970\n",
      "n= 5:  0.947  |  0.988   |  0.938   |  0.970\n",
      "n= 6:  0.939  |  0.989   |  0.929   |  0.970\n",
      "n= 7:  0.934  |  0.990   |  0.928   |  0.970\n",
      "n= 8:  0.947  |  0.991   |  0.927   |  0.971\n",
      "n= 9:  0.949  |  0.990   |  0.925   |  0.972\n",
      "\n",
      "Average cross validation scores (f1)...\n",
      "     dist_n   | dist_0_n | prox_n   | prox_0_n \n",
      "              |          |          |          \n",
      "n= 0:  0.946  |  0.946   |  0.959   |  0.959\n",
      "n= 1:  0.954  |  0.977   |  0.962   |  0.968\n",
      "n= 2:  0.961  |  0.981   |  0.961   |  0.971\n",
      "n= 3:  0.953  |  0.982   |  0.949   |  0.972\n",
      "n= 4:  0.949  |  0.986   |  0.944   |  0.971\n",
      "n= 5:  0.950  |  0.988   |  0.938   |  0.971\n",
      "n= 6:  0.941  |  0.990   |  0.929   |  0.971\n",
      "n= 7:  0.936  |  0.991   |  0.928   |  0.972\n",
      "n= 8:  0.949  |  0.992   |  0.928   |  0.972\n",
      "n= 9:  0.951  |  0.991   |  0.925   |  0.973\n"
     ]
    }
   ],
   "source": [
    "featFunListStr = ['dist_n', 'dist_0_n', 'prox_n', 'prox_0_n']\n",
    "\n",
    "print(\"Average accuracy for majority class: {:4.3f}\".format(cvalScore_majClass))\n",
    "\n",
    "meanScores = np.mean(cvalScoreArr_acc,axis=-1)\n",
    "print()\n",
    "print(\"Average cross validation scores (accuracy)...\")\n",
    "print(\"     {:8s} | {:8s} | {:8s} | {:8s} \".format(*tuple(featFunListStr)) )\n",
    "print(\"              |          |          |          \")\n",
    "for i1 in range(nArr.size):\n",
    "    printStr = \"n={:2d}:\".format(nArr[i1]) + \\\n",
    "            \"  {:4.3f}  |\".format(meanScores[0,i1]) + \\\n",
    "            \"  {:4.3f}   |\".format(meanScores[1,i1]) + \\\n",
    "            \"  {:4.3f}   |\".format(meanScores[2,i1]) + \\\n",
    "            \"  {:4.3f}\".format(meanScores[3,i1])\n",
    "            \n",
    "    print(printStr)\n",
    "\n",
    "meanScores = np.mean(cvalScoreArr_f1,axis=-1)\n",
    "print()\n",
    "print(\"Average cross validation scores (f1)...\")\n",
    "print(\"     {:8s} | {:8s} | {:8s} | {:8s} \".format(*tuple(featFunListStr)) )\n",
    "print(\"              |          |          |          \")\n",
    "for i1 in range(nArr.size):\n",
    "    printStr = \"n={:2d}:\".format(nArr[i1]) + \\\n",
    "            \"  {:4.3f}  |\".format(meanScores[0,i1]) + \\\n",
    "            \"  {:4.3f}   |\".format(meanScores[1,i1]) + \\\n",
    "            \"  {:4.3f}   |\".format(meanScores[2,i1]) + \\\n",
    "            \"  {:4.3f}\".format(meanScores[3,i1])\n",
    "            \n",
    "    print(printStr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some very good numbers indeed. The problem is, this makes it hard to pick out one particular set of features. The accuracy and f1 score are all quite high. \n",
    "\n",
    "The best models seem to be the dist_0_n and prox_0_n for n=10. That's 90 features. Making sense of 90 coefficients is a bit tedious. Let's look instead at dist_n and prox_n coefficients for different n.\n",
    "\n",
    "We don't need cross validation this time. Just a straight-forward model with the full training data, and its associated coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:00<00:00, 14.29it/s]\n",
      "100%|| 10/10 [00:00<00:00, 10.80it/s]\n",
      "100%|| 10/10 [00:05<00:00,  1.25it/s]\n",
      "100%|| 10/10 [00:05<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score, jaccard_similarity_score, log_loss\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "thres = 0.5  # Threshold to use for classification\n",
    "nArr = np.arange(n_nbrs)\n",
    "featFunList = [dist_n, prox_n, dist_0_n, prox_0_n]\n",
    "\n",
    "# Let's do testing as well\n",
    "testScoreArr_acc = np.zeros((len(featFunList), nArr.size))\n",
    "testScoreArr_f1 = np.zeros((len(featFunList), nArr.size))\n",
    "testScoreArr_ll = np.zeros((len(featFunList), nArr.size))\n",
    "confuseMatList = []\n",
    "coeffArr = np.zeros((2,nArr.size, len(venueCats_sans_sports)+1))\n",
    "\n",
    "funCtr = -1\n",
    "\n",
    "\n",
    "for featFun in featFunList:\n",
    "    funCtr += 1\n",
    "    for i1 in tqdm(range(nArr.size)):\n",
    "        n = nArr[i1]\n",
    "\n",
    "        X = featureVector(df_train, featureFun=featFun, n=n)[~dropInds,:]\n",
    "        y = y0[~dropInds]\n",
    "        yBool = (y>=thres).astype(int)\n",
    "        # Label is always for closest sports facility, l could vary\n",
    "        \n",
    "        logisticModel = LogisticRegression(penalty='l1',solver='liblinear').fit(X,yBool)\n",
    "        if funCtr < 2:\n",
    "            coeffArr[funCtr, i1,0] = logisticModel.intercept_\n",
    "            coeffArr[funCtr, i1,1:] = logisticModel.coef_.flatten()\n",
    "        \n",
    "        X_test = featureVector(df_test, featureFun=featFun, n=n) # Don't drop anything. Use full set\n",
    "        y_test = labelArr(df_test)\n",
    "        yBool_test = (y_test>=thres).astype(int)\n",
    "        \n",
    "        predictions = logisticModel.predict(X_test)\n",
    "        prob_predictions = logisticModel.predict_proba(X_test)\n",
    "        \n",
    "        testScoreArr_acc[funCtr,i1] = jaccard_similarity_score(yBool_test, predictions)\n",
    "        testScoreArr_f1[funCtr,i1] = f1_score(yBool_test, predictions)\n",
    "        testScoreArr_ll[funCtr,i1] = log_loss(yBool_test, prob_predictions)\n",
    "        confuseMatList.append(confusion_matrix(yBool_test, predictions))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients for dist_n:\n",
      "     intercept    | banks        | cinemas      | colleges     | gyms         | hospitals    | pubs         | restaurants  | schools      | supermarkets\n",
      "n= 0        5.243 |       -0.968 |        0.012 |       -0.885 |       -0.620 |       -0.195 |        0.427 |       -0.072 |       -0.927 |       -0.178\n",
      "n= 1        5.554 |       -0.695 |       -0.110 |       -0.908 |       -1.136 |        0.172 |        0.715 |        0.042 |        0.452 |       -0.528\n",
      "n= 2        5.814 |       -0.454 |        0.100 |       -1.039 |       -1.536 |        0.166 |        1.115 |       -0.077 |        0.453 |       -0.521\n",
      "n= 3        5.111 |       -0.205 |        0.330 |       -0.837 |       -1.275 |        0.025 |        0.171 |        0.138 |        0.390 |       -0.312\n",
      "n= 4        5.582 |       -0.216 |        0.027 |       -0.581 |       -1.203 |        0.030 |        0.317 |        0.098 |        0.266 |       -0.235\n",
      "n= 5        5.871 |       -0.458 |        0.020 |       -0.594 |       -1.070 |        0.133 |        0.700 |       -0.166 |        0.392 |       -0.309\n",
      "n= 6        5.831 |       -0.342 |       -0.063 |       -0.500 |       -0.993 |        0.106 |        0.685 |       -0.089 |        0.301 |       -0.364\n",
      "n= 7        6.294 |       -0.316 |       -0.000 |       -0.531 |       -0.927 |        0.003 |        0.789 |       -0.087 |        0.406 |       -0.505\n",
      "n= 8        8.834 |        0.134 |       -0.866 |       -0.766 |       -1.245 |        0.240 |        0.987 |       -0.058 |       -0.009 |        0.000\n",
      "n= 9        8.504 |        0.149 |       -0.805 |       -0.730 |       -1.098 |        0.175 |        0.802 |       -0.026 |       -0.062 |        0.094\n",
      "\n",
      "\n",
      "Coefficients for prox_n:\n",
      "     intercept    | banks        | cinemas      | colleges     | gyms         | hospitals    | pubs         | restaurants  | schools      | supermarkets\n",
      "n= 0       -7.513 |        4.573 |        0.000 |        6.192 |        6.566 |        1.747 |       -3.077 |        2.128 |        2.976 |        4.011\n",
      "n= 1       -6.287 |        4.337 |       -7.705 |        7.799 |       12.028 |       -1.749 |       -5.149 |        0.571 |        0.000 |       24.793\n",
      "n= 2       -5.958 |        1.625 |       -9.449 |        8.275 |       20.204 |        0.000 |       -6.021 |        0.000 |        0.000 |       24.299\n",
      "n= 3       -5.735 |        3.243 |        0.000 |        7.749 |       20.812 |       -5.747 |       -9.668 |        0.000 |        0.000 |       27.697\n",
      "n= 4       -5.465 |        3.183 |        0.000 |        7.878 |       23.019 |       -7.388 |      -12.090 |        0.000 |        0.452 |       29.970\n",
      "n= 5       -5.458 |        5.738 |        0.000 |        7.257 |       22.455 |       -3.238 |      -17.975 |        0.000 |        0.000 |       34.599\n",
      "n= 6       -5.199 |        1.830 |        0.000 |        8.667 |       15.900 |        0.000 |        0.000 |       -4.566 |        0.000 |       26.994\n",
      "n= 7       -5.391 |        0.000 |        0.000 |        9.879 |       25.204 |        0.000 |        0.000 |       -5.517 |        0.000 |       22.086\n",
      "n= 8       -5.455 |        0.000 |        0.000 |       11.140 |       28.428 |        0.000 |        0.000 |       -6.687 |        0.277 |       18.948\n",
      "n= 9       -5.704 |       -1.648 |        0.000 |       11.227 |       40.043 |        0.000 |        0.000 |       -8.207 |        2.021 |        8.544\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients for dist_n:\")\n",
    "coeffNames = ['intercept'] + venueCats_sans_sports\n",
    "print(\"     {:12s} | {:12s} | {:12s} | {:12s} | {:12s} | {:12s} | {:12s} | {:12s} | {:12s} | {:12s}\".format(\n",
    "    *coeffNames) )\n",
    "for i1 in range(nArr.size):\n",
    "    n=nArr[i1]\n",
    "    print(\"n={:2d} {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f}\".format(n,\n",
    "        *tuple(coeffArr[0,i1])) )\n",
    "    \n",
    "print();print()\n",
    "print(\"Coefficients for prox_n:\")\n",
    "coeffNames = ['intercept'] + venueCats_sans_sports\n",
    "print(\"     {:12s} | {:12s} | {:12s} | {:12s} | {:12s} | {:12s} | {:12s} | {:12s} | {:12s} | {:12s}\".format(\n",
    "    *coeffNames) )\n",
    "for i1 in range(nArr.size):\n",
    "    n=nArr[i1]\n",
    "    print(\"n={:2d} {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f}\".format(n,\n",
    "        *tuple(coeffArr[1,i1])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for majority classifier: 0.797011207970112\n",
      "Metrics for dist_n:\n",
      "n =            0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9\n",
      "accuracy:  0.636 | 0.573 | 0.549 | 0.557 | 0.573 | 0.557 | 0.565 | 0.574 | 0.580 | 0.573\n",
      "f1 score:  0.517 | 0.480 | 0.466 | 0.473 | 0.481 | 0.470 | 0.475 | 0.479 | 0.479 | 0.473\n",
      "log-loss:  0.980 | 1.283 | 1.624 | 1.356 | 1.256 | 1.326 | 1.249 | 1.338 | 1.547 | 1.479\n",
      "\n",
      "\n",
      "Metrics for prox_n:\n",
      "n =            0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9\n",
      "accuracy:  0.729 | 0.687 | 0.653 | 0.656 | 0.655 | 0.649 | 0.682 | 0.672 | 0.666 | 0.654\n",
      "f1 score:  0.586 | 0.553 | 0.525 | 0.529 | 0.528 | 0.520 | 0.542 | 0.536 | 0.531 | 0.517\n",
      "log-loss:  1.029 | 1.616 | 1.782 | 1.791 | 1.741 | 1.744 | 1.510 | 1.585 | 1.588 | 1.634\n",
      "\n",
      "\n",
      "Metrics for dist_0_n:\n",
      "n =            0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9\n",
      "accuracy:  0.636 | 0.555 | 0.543 | 0.544 | 0.550 | 0.540 | 0.550 | 0.554 | 0.555 | 0.550\n",
      "f1 score:  0.517 | 0.470 | 0.464 | 0.466 | 0.470 | 0.464 | 0.470 | 0.474 | 0.474 | 0.471\n",
      "log-loss:  0.980 | 1.752 | 2.151 | 2.321 | 2.292 | 2.427 | 2.620 | 2.776 | 2.664 | 2.781\n",
      "\n",
      "\n",
      "Metrics for prox_0_n:\n",
      "n =            0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9\n",
      "accuracy:  0.729 | 0.699 | 0.700 | 0.700 | 0.697 | 0.699 | 0.700 | 0.695 | 0.696 | 0.695\n",
      "f1 score:  0.586 | 0.562 | 0.566 | 0.567 | 0.564 | 0.565 | 0.567 | 0.562 | 0.563 | 0.565\n",
      "log-loss:  1.029 | 1.569 | 1.620 | 1.602 | 1.602 | 1.605 | 1.597 | 1.594 | 1.588 | 1.579\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for majority classifier:\", maj_acc)\n",
    "\n",
    "print(\"Metrics for dist_n:\")\n",
    "print(\"n =        {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d}\".format(*nArr))\n",
    "print(\"accuracy:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_acc[0]))\n",
    "print(\"f1 score:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_f1[0]))\n",
    "print(\"log-loss:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_ll[0]))\n",
    "\n",
    "print();print()\n",
    "print(\"Metrics for prox_n:\")\n",
    "print(\"n =        {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d}\".format(*nArr))\n",
    "print(\"accuracy:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_acc[1]))\n",
    "print(\"f1 score:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_f1[1]))\n",
    "print(\"log-loss:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_ll[1]))\n",
    "\n",
    "print();print()\n",
    "print(\"Metrics for dist_0_n:\")\n",
    "print(\"n =        {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d}\".format(*nArr))\n",
    "print(\"accuracy:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_acc[2]))\n",
    "print(\"f1 score:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_f1[2]))\n",
    "print(\"log-loss:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_ll[2]))\n",
    "\n",
    "print();print()\n",
    "print(\"Metrics for prox_0_n:\")\n",
    "print(\"n =        {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d}\".format(*nArr))\n",
    "print(\"accuracy:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_acc[3]))\n",
    "print(\"f1 score:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_f1[3]))\n",
    "print(\"log-loss:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_ll[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the features drop out for prox_n, but none for dist_n. However, the 'n' where these features drop out isn't consistent. Cinemas seem to be unimportant. Hospitals, pubs, and schools also drop out quite often. Banks drop out only for n=7,8. \n",
    "\n",
    "Let's do this again, this time with a smaller set of venue categories: \n",
    "\n",
    "__banks, colleges, gyms, and supermarkets__\n",
    "\n",
    "We'll use two kinds of features as before: dist_n, prox_n. We will use a balanced dataset for training, and then test over the full test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define a new feature extraction function to use only the four venue categories:\n",
    "venueCats_small = ['banks', 'colleges', 'gyms', 'supermarkets']\n",
    "venueCats_small = sorted(venueCats_small)\n",
    "\n",
    "def featureVector_small(dfTmp,vCats_small, featureFun=prox_n, **kwargs):\n",
    "    \"\"\" Returns vector containing features associated to a location \n",
    "    Positional arguments:\n",
    "        dfTmp: DataFrame containing distances to n_nbrs nearest neighbors\n",
    "        vCats_small: A subset of venueCats to use a limited set of features\n",
    "    Keyword arguments:\n",
    "        featureFun : Callable that takes arguments (venueCat, **kwargs) to define features\n",
    "                        Defaults to feature_prox_n                    \n",
    "        **kwargs : Passed directly to featureFun\n",
    "    \n",
    "    Returns:\n",
    "        featureVec : np.ndarray of shape ( len(dfTmp), m*N ), where m is number of floats returned by featureFun,\n",
    "                        N is len(venueCats)-1\n",
    "    \"\"\"\n",
    "    assert vCats_small == sorted(vCats_small, key=str.lower)  \n",
    "    # Ensure venueCats_sans_sports properly ordered\n",
    "    \n",
    "       \n",
    "    featureArr0 = featureFun(dfTmp,vCats_small[0], **kwargs) # Features for category 0\n",
    "    assert isinstance(featureArr0, np.ndarray) and featureArr0.ndim == 2\n",
    "    m = featureArr0.shape[1]\n",
    "    featureArr = np.zeros( (len(dfTmp), m * (len(vCats_small)) ) )  # Initialize array for all cats,except sports\n",
    "    featureArr[:,:m] = featureArr0 # Assign features for cat 0\n",
    "    \n",
    "    # Assign features for other categories\n",
    "    for index in range(1, len(vCats_small)):\n",
    "        featureArr[:,index * m : (index+1) * m ] = featureFun(dfTmp,\n",
    "                                vCats_small[index], **kwargs)\n",
    "    \n",
    "    return featureArr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:00<00:00, 42.80it/s]\n",
      "100%|| 10/10 [00:00<00:00, 22.71it/s]\n",
      "100%|| 10/10 [00:03<00:00,  1.75it/s]\n",
      "100%|| 10/10 [00:02<00:00,  4.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score, jaccard_similarity_score, log_loss\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "thres = 0.5  # Threshold to use for classification\n",
    "nArr = np.arange(n_nbrs)\n",
    "featFunList = [dist_n, prox_n, dist_0_n, prox_0_n]\n",
    "\n",
    "# Let's do testing as well\n",
    "testScoreArr_acc = np.zeros((len(featFunList), nArr.size))\n",
    "testScoreArr_f1 = np.zeros((len(featFunList), nArr.size))\n",
    "testScoreArr_ll = np.zeros((len(featFunList), nArr.size))\n",
    "confuseMatList = []\n",
    "coeffArr = np.zeros((2,nArr.size, len(venueCats_small)+1))\n",
    "\n",
    "funCtr = -1\n",
    "\n",
    "\n",
    "for featFun in featFunList:\n",
    "    funCtr += 1\n",
    "    for i1 in tqdm(range(nArr.size)):\n",
    "        n = nArr[i1]\n",
    "\n",
    "        X = featureVector_small(df_train, venueCats_small,  featureFun=featFun, n=n)[~dropInds,:]\n",
    "        y = y0[~dropInds]\n",
    "        yBool = (y>=thres).astype(int)\n",
    "        # Label is always for closest sports facility, l could vary\n",
    "        \n",
    "        logisticModel = LogisticRegression(penalty='l1',solver='liblinear').fit(X,yBool)\n",
    "        if funCtr < 2:\n",
    "            coeffArr[funCtr, i1,0] = logisticModel.intercept_\n",
    "            coeffArr[funCtr, i1,1:] = logisticModel.coef_.flatten()\n",
    "        \n",
    "        X_test = featureVector_small(df_test, venueCats_small, featureFun=featFun, n=n) # Don't drop anything. Use full set\n",
    "        y_test = labelArr(df_test)\n",
    "        yBool_test = (y_test>=thres).astype(int)\n",
    "        \n",
    "        predictions = logisticModel.predict(X_test)\n",
    "        prob_predictions = logisticModel.predict_proba(X_test)\n",
    "        \n",
    "        testScoreArr_acc[funCtr,i1] = jaccard_similarity_score(yBool_test, predictions)\n",
    "        testScoreArr_f1[funCtr,i1] = f1_score(yBool_test, predictions)\n",
    "        testScoreArr_ll[funCtr,i1] = log_loss(yBool_test, prob_predictions)\n",
    "        confuseMatList.append(confusion_matrix(yBool_test, predictions))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients for dist_n:\n",
      "     intercept    | banks        | colleges     | gyms         | supermarkets\n",
      "n= 0        4.675 |       -1.000 |       -1.063 |       -0.673 |       -0.146\n",
      "n= 1        5.451 |       -0.336 |       -0.959 |       -0.718 |       -0.181\n",
      "n= 2        5.676 |       -0.236 |       -0.775 |       -0.790 |       -0.098\n",
      "n= 3        5.776 |       -0.047 |       -0.783 |       -0.889 |       -0.036\n",
      "n= 4        5.616 |       -0.015 |       -0.675 |       -0.895 |        0.006\n",
      "n= 5        5.567 |       -0.072 |       -0.682 |       -0.710 |       -0.016\n",
      "n= 6        5.529 |        0.000 |       -0.620 |       -0.714 |       -0.023\n",
      "n= 7        5.691 |        0.090 |       -0.625 |       -0.695 |       -0.046\n",
      "n= 8        5.852 |        0.195 |       -0.747 |       -0.673 |       -0.039\n",
      "n= 9        5.915 |        0.221 |       -0.704 |       -0.723 |       -0.017\n",
      "\n",
      "\n",
      "Coefficients for prox_n:\n",
      "     intercept    | banks        | colleges     | gyms         | supermarkets\n",
      "n= 0       -6.936 |        5.344 |        6.895 |        6.763 |        4.150\n",
      "n= 1       -6.448 |        3.733 |        7.435 |        8.735 |       15.398\n",
      "n= 2       -6.140 |        1.646 |        8.310 |       15.630 |       13.039\n",
      "n= 3       -5.737 |        1.677 |        8.022 |       15.132 |       19.134\n",
      "n= 4       -5.428 |        1.529 |        8.775 |       12.701 |       23.266\n",
      "n= 5       -5.371 |        2.739 |        8.801 |       11.153 |       26.259\n",
      "n= 6       -5.273 |        0.513 |        9.557 |       14.862 |       26.118\n",
      "n= 7       -5.480 |        0.000 |       10.432 |       21.583 |       22.231\n",
      "n= 8       -5.541 |       -0.279 |       12.095 |       23.417 |       20.538\n",
      "n= 9       -5.695 |       -2.095 |       12.914 |       35.826 |       10.112\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients for dist_n:\")\n",
    "coeffNames = ['intercept'] + venueCats_small\n",
    "print(\"     {:12s} | {:12s} | {:12s} | {:12s} | {:12s}\".format(\n",
    "    *coeffNames) )\n",
    "for i1 in range(nArr.size):\n",
    "    n=nArr[i1]\n",
    "    print(\"n={:2d} {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f}\".format(n,\n",
    "        *tuple(coeffArr[0,i1])) )\n",
    "    \n",
    "print();print()\n",
    "print(\"Coefficients for prox_n:\")\n",
    "coeffNames = ['intercept'] + venueCats_small\n",
    "print(\"     {:12s} | {:12s} | {:12s} | {:12s} | {:12s}\".format(\n",
    "    *coeffNames) )\n",
    "for i1 in range(nArr.size):\n",
    "    n=nArr[i1]\n",
    "    print(\"n={:2d} {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f} | {:12.3f}\".format(n,\n",
    "        *tuple(coeffArr[1,i1])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define majority classifier for comparison\n",
    "X = featureVector_small(df_train, venueCats_small, featureFun=dist_n, n=n) # Doesn't matter for majority classifier\n",
    "y = labelArr(df_train)\n",
    "yBool = (y>=thres).astype(int)\n",
    "majClass = DummyClassifier(strategy='most_frequent').fit(X, yBool)\n",
    "# Label is always for closest sports facility, l could vary\n",
    "\n",
    "predictions = majClass.predict(X_test)\n",
    "maj_acc = jaccard_similarity_score(yBool_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for majority classifier: 0.797011207970112\n",
      "Metrics for dist_n:\n",
      "n =            0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9\n",
      "accuracy:  0.648 | 0.613 | 0.597 | 0.590 | 0.588 | 0.573 | 0.580 | 0.574 | 0.584 | 0.583\n",
      "f1 score:  0.524 | 0.504 | 0.489 | 0.487 | 0.490 | 0.475 | 0.479 | 0.474 | 0.478 | 0.477\n",
      "log-loss:  0.888 | 1.052 | 1.138 | 1.147 | 1.120 | 1.093 | 1.083 | 1.104 | 1.111 | 1.112\n",
      "\n",
      "\n",
      "Metrics for prox_n:\n",
      "n =            0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9\n",
      "accuracy:  0.724 | 0.696 | 0.686 | 0.676 | 0.670 | 0.681 | 0.684 | 0.675 | 0.671 | 0.659\n",
      "f1 score:  0.575 | 0.560 | 0.550 | 0.541 | 0.533 | 0.541 | 0.542 | 0.538 | 0.534 | 0.518\n",
      "log-loss:  0.996 | 1.416 | 1.492 | 1.599 | 1.521 | 1.505 | 1.500 | 1.561 | 1.559 | 1.605\n",
      "\n",
      "\n",
      "Metrics for dist_0_n:\n",
      "n =            0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9\n",
      "accuracy:  0.648 | 0.623 | 0.623 | 0.618 | 0.611 | 0.605 | 0.604 | 0.614 | 0.603 | 0.609\n",
      "f1 score:  0.524 | 0.512 | 0.512 | 0.509 | 0.505 | 0.501 | 0.500 | 0.508 | 0.499 | 0.505\n",
      "log-loss:  0.887 | 1.116 | 1.195 | 1.221 | 1.257 | 1.289 | 1.349 | 1.432 | 1.472 | 1.495\n",
      "\n",
      "\n",
      "Metrics for prox_0_n:\n",
      "n =            0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9\n",
      "accuracy:  0.724 | 0.702 | 0.697 | 0.696 | 0.696 | 0.696 | 0.696 | 0.699 | 0.701 | 0.699\n",
      "f1 score:  0.575 | 0.565 | 0.561 | 0.561 | 0.561 | 0.561 | 0.561 | 0.563 | 0.565 | 0.565\n",
      "log-loss:  0.996 | 1.410 | 1.410 | 1.418 | 1.419 | 1.417 | 1.418 | 1.413 | 1.425 | 1.443\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for majority classifier:\", maj_acc)\n",
    "\n",
    "print(\"Metrics for dist_n:\")\n",
    "print(\"n =        {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d}\".format(*nArr))\n",
    "print(\"accuracy:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_acc[0]))\n",
    "print(\"f1 score:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_f1[0]))\n",
    "print(\"log-loss:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_ll[0]))\n",
    "\n",
    "print();print()\n",
    "print(\"Metrics for prox_n:\")\n",
    "print(\"n =        {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d}\".format(*nArr))\n",
    "print(\"accuracy:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_acc[1]))\n",
    "print(\"f1 score:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_f1[1]))\n",
    "print(\"log-loss:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_ll[1]))\n",
    "\n",
    "print();print()\n",
    "print(\"Metrics for dist_0_n:\")\n",
    "print(\"n =        {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d}\".format(*nArr))\n",
    "print(\"accuracy:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_acc[2]))\n",
    "print(\"f1 score:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_f1[2]))\n",
    "print(\"log-loss:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_ll[2]))\n",
    "\n",
    "print();print()\n",
    "print(\"Metrics for prox_0_n:\")\n",
    "print(\"n =        {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d} | {:5d}\".format(*nArr))\n",
    "print(\"accuracy:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_acc[3]))\n",
    "print(\"f1 score:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_f1[3]))\n",
    "print(\"log-loss:  {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f} | {:.3f}\".format(\n",
    "    *testScoreArr_ll[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The logistic regression models are hard to compare amongst different types of features. When we include a lot of random locations, the training/validation set inevitably ends up with lots of unsuitable locations ($S<0.5$). This makes the dataset highly unbalanced. When we try to balance this data using greater weighting for suitable locations, the resulting model has lower accuracy than a majority classifier. If we drop some of the datapoints so that the classes (suitable vs unsuitable locations) are balanced, the model has very high training accuracy. When tested on unbalanced data, the testing accuracy does suffer. If we altogether ignore the data imbalance during training, then the precision and recall for suitable locations are abysmal. \n",
    "\n",
    "The choice of feature doesn't impact the models very much. Using distance vs proximity, or $n^{th}$ closest venue vs $\\{0,..,n\\}$ closest venues doesn't make too much of a difference. Using proximity does discard more features. In any case, the models don't tell us anything particularly surprising. When proximity is used for features, the intercept is negative, and venue proximities have positive coefficients. The signs are inverted when distance is used for features. The magnitude of the coefficients change as 'n' changes. \n",
    "\n",
    "Ultimately, all we learn from these models is that sports facilities tend to be suitable in locations close to other venues. Is this interesting? Perhaps. Is the model acceptable? It's hard to say as far as accuracy is concerned. Does it satisfy my eagerness to find an elegant method to find suitable locations. __NO.__\n",
    "\n",
    "I will have another go at it. This time, I will not build a binary classifier, but a plain old linear regression model where the target is a real number (between 0 and 1). We won't have to worry about majority and minority classes and probabilities. Just the mean squared errors and such. I will do this in the next notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
