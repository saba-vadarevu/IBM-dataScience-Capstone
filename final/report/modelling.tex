\chapter{Modelling}

\subsection{Train-Test Split}
It's best to always set aside a part of the dataset for testing. In our case, I've left 10\% of the data aside for testing. That's 803 points. Not a bad number for testing. 


\subsection{Targets}
{\color{blue} Alright! Let's get training.}

After just one little thing. There are two ways to do machine learning. Supervised and unsupervised. We will do supervised learning first. That means, we give the model a target to aim at. In unsupervised training, there isn't any target to aim at; we just want the model to group together different datapoints that look similar. 

{\color{blue} What target do we want our model to be aiming at?}

That is an important question. We want the target to reflect the suitability of a location for setting up a sports facility. Let's call this a ``suitability score''. We will have this suitablity score be between 0 and 1. If the suitability score is 0, then the location is unsuitable. If the suitability score is 1, then the location is highly suitable. 

{\color{blue} Okay. What exactly is this suitability score?}

That remains to be defined. I came up with this simple definition:
\begin{equation}
	\begin{aligned}
		p_n(\text{sports}) &= \frac{1}{1+\frac{d_n(\text{sports})}{0.5}}, \\
		S &= p_0(\text{sports}),
	\end{aligned}
\end{equation}
where $d_n(\text{sports})$ is the distance to the $n^{th}$ closest sports venue, $p_n(\text{sports})$ is the `proximity' to the $n^{th}$ closest sports venue defined above, and $S$ is the suitability score. The suitability score is based on proximity to just to closest sports venue. Note that the subscript $n$ is zero-based, so a subscript zero means first closest venue. 

For any location that coincides with the location of an existing sports facility, the suitability score is 1. 

{\color{blue} I think I understand what's going on. Proximity is defined so that it always takes values between 0 and 1. Why do you have the factor $0.5$ though?}

That acts to scale the distances. Basically, it makes proximity go to $0.5$ when the distance to the $n^{th}$ nearest venue is 0.5km. With regards to the suitability score, this means the score drops to 0.5 when we move to a location 0.5km away from an existing sports facility. It just feels nicer to have a scaling that makes sense to me. You are free to use a different scaling if you want. 

{\color{blue} Would the results of the modelling change if we use different scaling factors?}

Yes, they would. 

{\color{blue} And let me guess. You don't want to try different scaling factors because I'm not paying you to do this.}

Yep. I have plotted the locations in the test set, colored by their suitability score, in fig. \ref{fig:datapoints-test}. Locations closer to the city center have greater suitability scores in general, although there are quite a few towards the edges as well. 

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{datapoints_test.png}
	\caption{Locations of datapoints in the test set, colored by suitability scores $S$. Blue is for $S=0$, and red for $S=1$. \label{fig:datapoints-test}}
\end{figure}



\subsection{Features}
{\color{blue} We have a target to aim at. From what I understand, we also need some input to go into the model, yes?}


Yes, we do. These inputs are called features. Usually, a lot of work goes into deciding what features to use. In fact, that is the reason we collected distances to 10 nearest venues for 9 different venue categories. We are hoping that all of these distances can provide a meaningful set of features that can adequately characterize neighborhoods. Should we use all of the 10 distances, or just a few? Or should we use `proximity' defined above? After all, proximite goes from 0 to 1, which is a nice property to have. 

{\color{blue} Why are you asking me? I am absolutely clueless about what to use.}

Unfortunately, so am I. We will try them all out. Specifically, I have defined four different functions, dist\_n, dist\_0\_n, prox\_n, and prox\_0\_n. These functions use the 9 sets of 10 distances for each location and spit out a vector. See \href{https://github.com/saba-vadarevu/IBM-dataScience-Capstone/blob/master/final/model_binary.ipynb}{this notebook} for details.  
We will try to use feature vectors produced by each of these four functions and see which one gives us the best accuracy. 


\section{Binary classification}

{\color{blue} The notebook is called `model\_binary'. What's with the binary?}

The binary refers to binary classification. In this kind of modelling, we say that our data belongs to two classes -- suitable locations, and unsuitable locations. We have to assign these labels when training our data. 

{\color{blue} Let me guess. Locations with suitability scores less than 0.5 are unsuitable, and those with suitability scores greater than 0.5 are suitable?}

Yep. To be precise, those with suitability score greater than \textbf{or equal to} 0.5 are suitable. There is this library called Scikit-learn that has models ready to go. I will spare you the details of getting these models to run. Suffice it to say that I tried all four feature functions for different `n', where `n' is the rank of the venue in terms of distance from the location. Some feature functions did better than others, but the overall performance wasn't so good.

{\color{blue} Really?! I was getting excited about this. It sounded like it should really work.}

Well, our problem just doesn't happen to suit binary classification. Remember how we collected a lot of random locations in the city to build a dataset? Most of these locations fall far from existing sports facilities, and are consequently labelled as unsuitable. There are a lot more of these locations than suitable locations. This imbalance makes it quite messy to have a reasonable accuracy with the model. 

{\color{blue} Can't we just drop those extra points?}

We can, but then the dataset we have will be very biased in terms of what neighborhoods it represents. We can't take a model trained on this dataset and expect it to make predictions on random locations in the city. 

{\color{blue} How about adding more datapoints near existing sports facilities?}

We could do that. But then all of the models we train will have a large error associated with them, and it becomes hard to compare models. 

{\color{blue} So there's no way to fix this?}

I'm not sure. To be honest, I just don't want to do this binary classification. Ultimately, we want don't want just any suitable location, we want the most suitable location. That is, we want our locations to be ranked in terms of suitability scores. If we are aiming for a continuous target to facilitate ranking, then using binary classification isn't the best idea. 

{\color{blue} Didn't you have some probabilities associated with the predictions?}

Yes, there are probabilities associated with the predictions. And you are right, we can find the locations with the greatest probability. It just feels a bit too artificial though, to assign discrete labels, and then rank them on probability. We might as well skip this step and just work with continuous targets all the way. 

{\color{blue} Alright. What's the next step then?}


\section{Linear Regression}

I'm feeling a bit under the weather today. But don't worry, I got the job done. Go read it all in \href{https://github.com/saba-vadarevu/IBM-dataScience-Capstone/blob/master/final/model_continuous.ipynb}{this notebook}

{\color{blue} Sure. Should you still be in a pub if you're sick?}

I'm not that sick. And a bit of booze tends to make me feel better. 

{\color{blue} Yea, right. Why do I feel like you're just tired of explaining this to me?}

Are you saying that after writing all of that code and all of those comments and remarks in the notebook, I can't be bothered now to explain it all to you? Of course not! I love repeating stuff I've already written somewhere else. It's just that I'm a bit tired today. 

{\color{blue} Okay, fine. I understand. I'll just go read that notebook thingy.

What is this $R^2$ that keeps appearing in the notebook?}

That is a measure of the model's accuracy. It tells you how much of the true target's variation about its mean is explained by the model. An $R^2$ of 1 means that the model perfectly explains the variability. An $R^2$ of 0 means the model explains none of it and is absolute trash. 

{\color{blue} Let me see. We got to an $R^2$ of 0.5 for the simple linear regression. That means half of the variability is explained. Is that good enough?}

Maybe. It's hard to tell, really. Unless we have some other model to compare against. In any case, $R^2=0.5$ isn't particularly bad. 

{\color{blue} Do we have some other model to compare against then?}

Yup. We'll use a few feedforward neural networks to see if we can do better.




\section{Feedforward Neural Networks}

{\color{blue} Ooh. Neural networks. I always wanted to get in on neural networks. What do they do?}

Don't get too excited. They're not all that different from simple linear regression. We just have lots of different units that each do their own simple linear regression. Then we pass the output of this regression through a simple non-linear function. The idea is that putting together a bunch of these units will help us capture a wide variety of non-linear functions without having to actually use a lot of non-linear functions. 

{\color{blue} I don't know if I should be more excited or less excited. Anyway, how do we do these neural networks?}

Existing libraries, of course. Scikit-learn, the one we used for linear regression, also allows using neural networks. We'll use the simplest case first, a single layer perceptron. Unfortunately, I'm a bit tired today as well, so just have a look at the notebook.

{\color{blue} Okay, I've had a read. So now we got to $R^2=0.74$ on the test set. That's significant increase from the linear regression. I suppose. }

Indeed it is. The predictions and the errors on the training and test sets are shown in fig. \ref{fig:MLP-preds-errors}. There could be other networks that provide better accuracies, but we'll stop with the Multilayer Perceptron with 3 hidden layers for now. We'll use this model for the optimization. 

\begin{figure}
	\centering
	\begin{subfigure}{\linewidth}
		\includegraphics[width = 0.95\linewidth]{MLP_preds.jpg}
		\caption{Predictions and true targets}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
		\includegraphics[width = 0.95\linewidth]{MLP_errors.jpg}
		\caption{Errors}
	\end{subfigure}
	\caption{Predictions and errors in the target values for the Multilayer perceptron with 3 hidden layers with (100,20,5) hidden units. \label{fig:MLP-preds-errors}}
\end{figure}


\section{Clustering}

{\color{blue} Didn't you say something about unsupervised learning earlier?}

Yes, I did. In unsupervised learning, we usually group together datapoints that ``look'' similar. This is called Clustering; there are other kinds of unsupervised learning, but we're not interested in those. Even clustering is not very relevant to our little project though. Remember what I said about wanting to rank locations by suitability? It is hard to do that with clustering. The best we can do is to find locations that somehow look similar to the locations of existing sports facilities. 

{\color{blue} Can we give it a go then?}

I did try doing it. Clustering works best when the datapoints we are interested in are neatly separated in terms of features from the other datapoints. Unfortunately, in our case, there isn't such nice separation. Things are all smooth and continuous, so it isn't very sensible to separate datapoints into different clusters. Have a look at fig. \ref{fig:cluster_4_prox_0} which shows clustering when proximity to nearest venue of each category is used as the feature vector. In the figure on the left, we would have liked to see short horizontal lines. The long lines mean that the clusters contain locations with a wide range of suitability scores, which isn't good news. 

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{cluster_4_prox_0.png}
	\caption{Clustering with K-Means, using 4 clusters and proximities to nearest venue of each category (except sports) as feature vector. \label{fig:cluster_4_prox_0}}
\end{figure}



{\color{blue} So.. We give up on this?}

Yep. We have the multilayer perceptron working for us though, so don't worry about this one. 
